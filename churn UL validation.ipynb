{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# ИМПОРТ БИБЛИОТЕК И КОНФИГУРАЦИЯ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# ИМПОРТ БИБЛИОТЕК\n",
    "# ====================================================================================\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import time\n",
    "import gc\n",
    "\n",
    "# Данные\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from scipy.stats import pointbiserialr\n",
    "\n",
    "# Визуализация\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ML (для импутации)\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Настройки\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.float_format', lambda x: '%.4f' % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# КОНФИГУРАЦИЯ\n",
    "# ====================================================================================\n",
    "\n",
    "class Config:\n",
    "    \"\"\"Централизованная конфигурация\"\"\"\n",
    "    \n",
    "    # ВОСПРОИЗВОДИМОСТЬ\n",
    "    RANDOM_SEED = 42\n",
    "    \n",
    "    # ПУТИ\n",
    "    DATA_DIR = Path(\"data\")\n",
    "    OUTPUT_DIR = Path(\"output\")\n",
    "    FIGURES_DIR = Path(\"figures\")\n",
    "    \n",
    "    # ФАЙЛЫ\n",
    "    TRAIN_FILE = \"churn_train_ul.parquet\"\n",
    "    \n",
    "    # КОЛОНКИ\n",
    "    ID_COLUMNS = ['cli_code', 'client_id', 'observation_point']\n",
    "    TARGET_COLUMN = 'target_churn_3m'\n",
    "    SEGMENT_COLUMN = 'segment_group'\n",
    "    DATE_COLUMN = 'observation_point'\n",
    "    CATEGORICAL_FEATURES = ['segment_group', 'obs_month', 'obs_quarter']\n",
    "    \n",
    "    # СЕГМЕНТЫ (ДВЕ МОДЕЛИ)\n",
    "    SEGMENT_1_NAME = \"Small Business\"\n",
    "    SEGMENT_1_VALUES = ['SMALL_BUSINESS']\n",
    "    \n",
    "    SEGMENT_2_NAME = \"Middle + Large Business\"\n",
    "    SEGMENT_2_VALUES = ['MIDDLE_BUSINESS', 'LARGE_BUSINESS']\n",
    "    \n",
    "    # ВРЕМЕННОЕ РАЗБИЕНИЕ\n",
    "    TRAIN_SIZE = 0.70\n",
    "    VAL_SIZE = 0.15\n",
    "    TEST_SIZE = 0.15\n",
    "    \n",
    "    # PREPROCESSING (как в исходном файле)\n",
    "    REMOVE_GAPS = True  # Gap detection\n",
    "    HANDLE_OUTLIERS = True  # IQR clipping\n",
    "    REMOVE_HIGH_CORRELATIONS = True  # Удаление коррелирующих признаков\n",
    "    CORRELATION_THRESHOLD = 0.85  # Порог корреляции между признаками\n",
    "    OUTLIER_IQR_MULTIPLIER = 1.5  # Множитель для IQR\n",
    "    \n",
    "    # CORRELATION ANALYSIS С TARGET\n",
    "    CORRELATION_P_VALUE_THRESHOLD = 0.05\n",
    "    DATA_LEAKAGE_THRESHOLD = 0.9\n",
    "    TOP_N_CORRELATIONS = 20\n",
    "    TOP_N_VISUALIZATION = 30\n",
    "    \n",
    "    @classmethod\n",
    "    def create_directories(cls):\n",
    "        for dir_path in [cls.OUTPUT_DIR, cls.FIGURES_DIR]:\n",
    "            dir_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    @classmethod\n",
    "    def get_train_path(cls):\n",
    "        return cls.DATA_DIR / cls.TRAIN_FILE\n",
    "\n",
    "config = Config()\n",
    "config.create_directories()\n",
    "np.random.seed(config.RANDOM_SEED)\n",
    "\n",
    "print(\"\\n✓ Конфигурация инициализирована\")\n",
    "print(f\"  Random seed: {config.RANDOM_SEED}\")\n",
    "print(f\"  Сегмент 1: {config.SEGMENT_1_NAME} {config.SEGMENT_1_VALUES}\")\n",
    "print(f\"  Сегмент 2: {config.SEGMENT_2_NAME} {config.SEGMENT_2_VALUES}\")\n",
    "print(f\"  Split: {config.TRAIN_SIZE}/{config.VAL_SIZE}/{config.TEST_SIZE}\")\n",
    "print(f\"\\nPREPROCESSING FLAGS:\")\n",
    "print(f\"  Gap detection: {config.REMOVE_GAPS}\")\n",
    "print(f\"  Outliers handling: {config.HANDLE_OUTLIERS} (IQR × {config.OUTLIER_IQR_MULTIPLIER})\")\n",
    "print(f\"  High correlations removal: {config.REMOVE_HIGH_CORRELATIONS} (threshold={config.CORRELATION_THRESHOLD})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# ЗАГРУЗКА ДАННЫХ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# ЗАГРУЗКА ДАННЫХ\n",
    "# ====================================================================================\n",
    "\n",
    "train_path = config.get_train_path()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ЗАГРУЗКА ДАННЫХ\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Файл: {train_path}\")\n",
    "\n",
    "if not train_path.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Файл не найден: {train_path}\\n\"\n",
    "        f\"Убедитесь, что файл {config.TRAIN_FILE} находится в папке data/\"\n",
    "    )\n",
    "\n",
    "file_size = train_path.stat().st_size / (1024**2)\n",
    "print(f\"Размер файла: {file_size:.2f} MB\")\n",
    "\n",
    "start = time.time()\n",
    "df_full = pd.read_parquet(train_path)\n",
    "load_time = time.time() - start\n",
    "\n",
    "memory = df_full.memory_usage(deep=True).sum() / (1024**2)\n",
    "\n",
    "print(f\"\\n✓ Загружено за {load_time:.2f} сек\")\n",
    "print(f\"  Размер: {df_full.shape}\")\n",
    "print(f\"  Память: {memory:.2f} MB\")\n",
    "print(f\"  Строк: {len(df_full):,}\")\n",
    "print(f\"  Колонок: {df_full.shape[1]}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# EXPLORATORY DATA ANALYSIS (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# БАЗОВАЯ ИНФОРМАЦИЯ О ДАННЫХ\n",
    "# ====================================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"БАЗОВАЯ ИНФОРМАЦИЯ О ДАННЫХ\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n1. РАЗМЕР ДАННЫХ:\")\n",
    "print(f\"   Строк: {len(df_full):,}\")\n",
    "print(f\"   Колонок: {df_full.shape[1]}\")\n",
    "print(f\"   Память: {df_full.memory_usage(deep=True).sum() / (1024**2):.2f} MB\")\n",
    "\n",
    "print(f\"\\n2. ТИПЫ ДАННЫХ:\")\n",
    "dtype_counts = df_full.dtypes.value_counts()\n",
    "for dtype, count in dtype_counts.items():\n",
    "    print(f\"   {dtype}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# АНАЛИЗ ЦЕЛЕВОЙ ПЕРЕМЕННОЙ\n",
    "# ====================================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"АНАЛИЗ ЦЕЛЕВОЙ ПЕРЕМЕННОЙ (TARGET)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Общий churn rate\n",
    "churn_rate = df_full[config.TARGET_COLUMN].mean()\n",
    "n_churned = df_full[config.TARGET_COLUMN].sum()\n",
    "n_total = len(df_full)\n",
    "ratio = (1-churn_rate)/churn_rate\n",
    "\n",
    "print(f\"\\n1. ОБЩИЙ CHURN RATE:\")\n",
    "print(f\"   Churn rate: {churn_rate:.4f} ({churn_rate*100:.2f}%)\")\n",
    "print(f\"   Churned: {n_churned:,}\")\n",
    "print(f\"   Not churned: {n_total - n_churned:,}\")\n",
    "print(f\"   Class ratio: 1:{ratio:.1f}\")\n",
    "\n",
    "# Churn rate по сегментам\n",
    "print(f\"\\n2. CHURN RATE ПО СЕГМЕНТАМ:\")\n",
    "for segment in df_full[config.SEGMENT_COLUMN].unique():\n",
    "    segment_df = df_full[df_full[config.SEGMENT_COLUMN] == segment]\n",
    "    seg_churn = segment_df[config.TARGET_COLUMN].mean()\n",
    "    seg_count = len(segment_df)\n",
    "    seg_pct = seg_count / len(df_full) * 100\n",
    "    print(f\"   {segment}:\")\n",
    "    print(f\"     Размер: {seg_count:,} ({seg_pct:.1f}%)\")\n",
    "    print(f\"     Churn rate: {seg_churn:.4f} ({seg_churn*100:.2f}%)\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# АНАЛИЗ ПРОПУЩЕННЫХ ЗНАЧЕНИЙ\n",
    "# ====================================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"АНАЛИЗ ПРОПУЩЕННЫХ ЗНАЧЕНИЙ\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "missing = df_full.isnull().sum()\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing': missing[missing > 0],\n",
    "    'Percent': (missing[missing > 0] / len(df_full) * 100).round(2)\n",
    "}).sort_values('Missing', ascending=False)\n",
    "\n",
    "print(f\"\\nКолонок с пропусками: {len(missing_df)}\")\n",
    "\n",
    "if len(missing_df) > 0:\n",
    "    print(f\"\\nТоп-20 колонок с наибольшим количеством пропусков:\")\n",
    "    print(missing_df.head(20))\n",
    "else:\n",
    "    print(\"\\n✓ Пропущенных значений не обнаружено\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# АНАЛИЗ КОНСТАНТНЫХ КОЛОНОК\n",
    "# ====================================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"АНАЛИЗ КОНСТАНТНЫХ КОЛОНОК\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "constant_cols = []\n",
    "for col in df_full.columns:\n",
    "    if df_full[col].nunique(dropna=False) == 1:\n",
    "        constant_cols.append(col)\n",
    "\n",
    "print(f\"\\nКонстантных колонок: {len(constant_cols)}\")\n",
    "\n",
    "if constant_cols:\n",
    "    print(f\"\\nСписок константных колонок:\")\n",
    "    for col in constant_cols:\n",
    "        print(f\"  - {col} (значение: {df_full[col].iloc[0]})\")\n",
    "else:\n",
    "    print(\"\\n✓ Константных колонок не обнаружено\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# АНАЛИЗ ВРЕМЕННОГО РАСПРЕДЕЛЕНИЯ\n",
    "# ====================================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"АНАЛИЗ ВРЕМЕННОГО РАСПРЕДЕЛЕНИЯ\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Конвертация даты\n",
    "df_full[config.DATE_COLUMN] = pd.to_datetime(df_full[config.DATE_COLUMN])\n",
    "\n",
    "print(f\"\\n1. ВРЕМЕННОЙ ПЕРИОД:\")\n",
    "print(f\"   Начало: {df_full[config.DATE_COLUMN].min().date()}\")\n",
    "print(f\"   Конец: {df_full[config.DATE_COLUMN].max().date()}\")\n",
    "print(f\"   Уникальных дат: {df_full[config.DATE_COLUMN].nunique()}\")\n",
    "\n",
    "print(f\"\\n2. КЛИЕНТЫ:\")\n",
    "print(f\"   Уникальных cli_code: {df_full['cli_code'].nunique():,}\")\n",
    "print(f\"   Уникальных client_id: {df_full['client_id'].nunique():,}\")\n",
    "\n",
    "# Распределение по датам\n",
    "print(f\"\\n3. РАСПРЕДЕЛЕНИЕ ЗАПИСЕЙ ПО ДАТАМ:\")\n",
    "date_dist = df_full.groupby(config.DATE_COLUMN).size()\n",
    "print(f\"   Среднее записей на дату: {date_dist.mean():.0f}\")\n",
    "print(f\"   Минимум: {date_dist.min():,}\")\n",
    "print(f\"   Максимум: {date_dist.max():,}\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# ВИЗУАЛИЗАЦИЯ: РАСПРЕДЕЛЕНИЕ TARGET\n",
    "# ====================================================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# 1. Общее распределение\n",
    "target_dist = df_full[config.TARGET_COLUMN].value_counts()\n",
    "axes[0].bar(['No Churn', 'Churn'], [target_dist[0], target_dist[1]],\n",
    "           color=['green', 'red'], alpha=0.7, edgecolor='black')\n",
    "axes[0].set_title('Общее распределение Target', fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylabel('Количество')\n",
    "axes[0].set_yscale('log')\n",
    "for i, v in enumerate([target_dist[0], target_dist[1]]):\n",
    "    axes[0].text(i, v, f'{v:,}\\n({v/len(df_full)*100:.2f}%)',\n",
    "                ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "# 2. По сегментам (stacked)\n",
    "segment_churn = df_full.groupby([config.SEGMENT_COLUMN, \n",
    "                                  config.TARGET_COLUMN]).size().unstack(fill_value=0)\n",
    "segment_churn.plot(kind='bar', stacked=True, ax=axes[1],\n",
    "                  color=['green', 'red'], alpha=0.7, edgecolor='black')\n",
    "axes[1].set_title('Распределение по сегментам', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Сегмент')\n",
    "axes[1].set_ylabel('Количество')\n",
    "axes[1].legend(['No Churn', 'Churn'], loc='upper right')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 3. Churn rate по сегментам\n",
    "churn_rates = df_full.groupby(config.SEGMENT_COLUMN)[config.TARGET_COLUMN].mean() * 100\n",
    "axes[2].bar(range(len(churn_rates)), churn_rates.values,\n",
    "           color='coral', alpha=0.7, edgecolor='black')\n",
    "axes[2].set_xticks(range(len(churn_rates)))\n",
    "axes[2].set_xticklabels(churn_rates.index, rotation=45, ha='right')\n",
    "axes[2].set_title('Churn Rate по сегментам', fontsize=14, fontweight='bold')\n",
    "axes[2].set_ylabel('Churn Rate (%)')\n",
    "for i, v in enumerate(churn_rates.values):\n",
    "    axes[2].text(i, v, f'{v:.2f}%', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(config.FIGURES_DIR / 'eda_target_distribution.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Сохранено: figures/eda_target_distribution.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ВРЕМЕННОЕ РАЗБИЕНИЕ (TEMPORAL SPLIT)\n",
    "\n",
    "Разбиение данных по времени для предотвращения data leakage:  \n",
    "- **Train:** 70% первых по времени\n",
    "- **Validation:** 15%\n",
    "- **Test (OOT):** 15% последних"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# TEMPORAL SPLIT\n",
    "# ====================================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ВРЕМЕННОЕ РАЗБИЕНИЕ (TEMPORAL SPLIT)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Сортировка по времени\n",
    "df_sorted = df_full.sort_values(config.DATE_COLUMN).reset_index(drop=True)\n",
    "unique_dates = sorted(df_sorted[config.DATE_COLUMN].unique())\n",
    "n_dates = len(unique_dates)\n",
    "\n",
    "print(f\"\\nУникальных дат: {n_dates}\")\n",
    "print(f\"Период: {unique_dates[0].date()} - {unique_dates[-1].date()}\")\n",
    "\n",
    "# Cutoff indices\n",
    "train_cutoff = int(n_dates * config.TRAIN_SIZE)\n",
    "val_cutoff = int(n_dates * (config.TRAIN_SIZE + config.VAL_SIZE))\n",
    "\n",
    "train_end = unique_dates[train_cutoff - 1]\n",
    "val_end = unique_dates[val_cutoff - 1]\n",
    "\n",
    "print(f\"\\nCutoff даты:\")\n",
    "print(f\"  Train: до {train_end.date()} ({train_cutoff} дат)\")\n",
    "print(f\"  Val: {unique_dates[train_cutoff].date()} - {val_end.date()} ({val_cutoff - train_cutoff} дат)\")\n",
    "print(f\"  Test (OOT): {unique_dates[val_cutoff].date()}+ ({n_dates - val_cutoff} дат)\")\n",
    "\n",
    "# Создание split\n",
    "train_df = df_sorted[df_sorted[config.DATE_COLUMN] <= train_end].copy()\n",
    "val_df = df_sorted[(df_sorted[config.DATE_COLUMN] > train_end) & \n",
    "                   (df_sorted[config.DATE_COLUMN] <= val_end)].copy()\n",
    "test_df = df_sorted[df_sorted[config.DATE_COLUMN] > val_end].copy()\n",
    "\n",
    "# Статистика по split\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"СТАТИСТИКА ПО SPLIT\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "for name, df in [('TRAIN', train_df), ('VALIDATION', val_df), ('TEST (OOT)', test_df)]:\n",
    "    churn_r = df[config.TARGET_COLUMN].mean()\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Записей: {len(df):,}\")\n",
    "    print(f\"  Клиентов (cli_code): {df['cli_code'].nunique():,}\")\n",
    "    print(f\"  Период: {df[config.DATE_COLUMN].min().date()} - {df[config.DATE_COLUMN].max().date()}\")\n",
    "    print(f\"  Churn rate: {churn_r:.4f} ({churn_r*100:.2f}%)\")\n",
    "    print(f\"  Процент от общего: {len(df)/len(df_full)*100:.2f}%\")\n",
    "\n",
    "# Проверка data leakage\n",
    "assert train_df[config.DATE_COLUMN].max() < val_df[config.DATE_COLUMN].min(), \"Data leakage detected!\"\n",
    "assert val_df[config.DATE_COLUMN].max() < test_df[config.DATE_COLUMN].min(), \"Data leakage detected!\"\n",
    "print(f\"\\n✓ Temporal ordering verified - NO DATA LEAKAGE\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# GAP DETECTION - УДАЛЕНИЕ КЛИЕНТОВ С ПРОБЕЛАМИ\n",
    "# ====================================================================================\n",
    "\n",
    "if config.REMOVE_GAPS:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"GAP DETECTION - УДАЛЕНИЕ КЛИЕНТОВ С ПРОБЕЛАМИ\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    print(\"\\nАнализ пробелов в train данных...\")\n",
    "\n",
    "    # Chunked processing для экономии памяти\n",
    "    unique_clients = train_df['cli_code'].unique()\n",
    "    chunk_size = 10000\n",
    "    clients_with_gaps_list = []\n",
    "\n",
    "    for i in range(0, len(unique_clients), chunk_size):\n",
    "        chunk_clients = unique_clients[i:i+chunk_size]\n",
    "        chunk = train_df[train_df['cli_code'].isin(chunk_clients)].copy()\n",
    "        chunk = chunk.sort_values(['cli_code', config.DATE_COLUMN])\n",
    "\n",
    "        # Конвертируем даты в месячные номера\n",
    "        chunk['month_num'] = chunk[config.DATE_COLUMN].dt.to_period('M').apply(lambda x: x.ordinal)\n",
    "        chunk['month_diff'] = chunk.groupby('cli_code')['month_num'].diff()\n",
    "\n",
    "        # Анализ пробелов\n",
    "        gaps = chunk.groupby('cli_code')['month_diff'].agg([\n",
    "            ('max_gap', 'max'),\n",
    "            ('total_gaps', lambda x: (x > 1).sum())\n",
    "        ]).reset_index()\n",
    "\n",
    "        chunk_gaps = gaps[gaps['max_gap'] > 1]\n",
    "        clients_with_gaps_list.append(chunk_gaps)\n",
    "\n",
    "        if (i // chunk_size + 1) % 10 == 0:\n",
    "            gc.collect()\n",
    "            print(f\"  Обработано {i+chunk_size:,}/{len(unique_clients):,} клиентов\")\n",
    "\n",
    "    clients_with_gaps = pd.concat(clients_with_gaps_list, ignore_index=True)\n",
    "\n",
    "    gap_pct = len(clients_with_gaps) / len(unique_clients) * 100\n",
    "    print(f\"\\nКлиентов с пробелами: {len(clients_with_gaps):,} ({gap_pct:.2f}%)\")\n",
    "\n",
    "    if len(clients_with_gaps) > 0:\n",
    "        bad_clients = set(clients_with_gaps['cli_code'])\n",
    "\n",
    "        train_before = len(train_df)\n",
    "        val_before = len(val_df)\n",
    "        test_before = len(test_df)\n",
    "        \n",
    "        train_df = train_df[~train_df['cli_code'].isin(bad_clients)].copy()\n",
    "        val_df = val_df[~val_df['cli_code'].isin(bad_clients)].copy()\n",
    "        test_df = test_df[~test_df['cli_code'].isin(bad_clients)].copy()\n",
    "\n",
    "        print(f\"\\nУдалено:\")\n",
    "        print(f\"  Train: {train_before:,} {len(train_df):,} (-{train_before - len(train_df):,})\")\n",
    "        print(f\"  Val: {val_before:,} {len(val_df):,} (-{val_before - len(val_df):,})\")\n",
    "        print(f\"  Test: {test_before:,} {len(test_df):,} (-{test_before - len(test_df):,})\")\n",
    "\n",
    "        del clients_with_gaps, bad_clients\n",
    "        gc.collect()\n",
    "    else:\n",
    "        print(\"\\n✓ Клиентов с пробелами не обнаружено\")\n",
    "\n",
    "    print(\"=\"*80)\n",
    "else:\n",
    "    print(\"\\n Gap detection отключен (config.REMOVE_GAPS=False)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# PREPROCESSING PIPELINE\n",
    "# ====================================================================================\n",
    "\n",
    "class PreprocessingPipeline:\n",
    "    \"\"\"Полный preprocessing pipeline из исходного файла\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.fitted_columns = None\n",
    "        self.final_features = None\n",
    "        self.constant_cols = []\n",
    "        self.outlier_bounds = {}\n",
    "        self.numeric_imputer = None\n",
    "        self.categorical_imputer = None\n",
    "        self.numeric_cols_for_imputation = []\n",
    "        self.categorical_cols_for_imputation = []\n",
    "        self.features_to_drop_corr = []\n",
    "\n",
    "    def fit_transform(self, train_df):\n",
    "        \"\"\"Fit and transform training data\"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"PREPROCESSING: FIT_TRANSFORM ON TRAIN\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "        df = train_df.copy()\n",
    "\n",
    "        # Store columns\n",
    "        self.fitted_columns = [c for c in df.columns\n",
    "                              if c not in config.ID_COLUMNS + [config.TARGET_COLUMN]]\n",
    "\n",
    "        # 1. Remove constants\n",
    "        df = self._remove_constants(df, fit=True)\n",
    "\n",
    "        # 2. Handle outliers (IQR clipping)\n",
    "        df = self._handle_outliers(df, fit=True)\n",
    "\n",
    "        # 3. Handle missing\n",
    "        df = self._handle_missing(df, fit=True)\n",
    "\n",
    "        # 4. Remove high correlations\n",
    "        df = self._remove_correlations(df, fit=True)\n",
    "\n",
    "        # Final features\n",
    "        self.final_features = [c for c in df.columns\n",
    "                              if c not in config.ID_COLUMNS + [config.TARGET_COLUMN]]\n",
    "\n",
    "        print(f\"\\n✓ Preprocessing complete\")\n",
    "        print(f\"  Final features: {len(self.final_features)}\")\n",
    "\n",
    "        return df\n",
    "\n",
    "    def transform(self, df, dataset_name='test'):\n",
    "        \"\"\"Transform new data\"\"\"\n",
    "        print(f\"\\nPreprocessing: {dataset_name}\")\n",
    "\n",
    "        df = df.copy()\n",
    "\n",
    "        df = self._remove_constants(df, fit=False)\n",
    "        df = self._handle_outliers(df, fit=False)\n",
    "        df = self._handle_missing(df, fit=False)\n",
    "        df = self._remove_correlations(df, fit=False)\n",
    "        df = self._align_columns(df, dataset_name)\n",
    "\n",
    "        print(f\"  ✓ {dataset_name}: {df.shape}\")\n",
    "\n",
    "        return df\n",
    "\n",
    "    def _remove_constants(self, df, fit):\n",
    "        if fit:\n",
    "            print(\"\\n1. Removing constant columns...\")\n",
    "            for col in df.columns:\n",
    "                if col in config.ID_COLUMNS + [config.TARGET_COLUMN]:\n",
    "                    continue\n",
    "                if df[col].nunique(dropna=False) == 1:\n",
    "                    self.constant_cols.append(col)\n",
    "\n",
    "            if self.constant_cols:\n",
    "                df = df.drop(columns=self.constant_cols)\n",
    "                print(f\"   Removed: {len(self.constant_cols)}\")\n",
    "            else:\n",
    "                print(f\"   ✓ No constant columns found\")\n",
    "        return df\n",
    "\n",
    "    def _handle_outliers(self, df, fit):\n",
    "        \"\"\"IQR-based outlier clipping для финансовых признаков\"\"\"\n",
    "        if not config.HANDLE_OUTLIERS:\n",
    "            return df\n",
    "\n",
    "        if fit:\n",
    "            print(\"\\n2. Handling outliers (IQR clipping)...\")\n",
    "            # Ключевые слова для финансовых признаков\n",
    "            keywords = ['profit', 'income', 'expense', 'margin', 'provision',\n",
    "                       'balance', 'assets', 'liabilities', 'revenue', 'cost']\n",
    "            \n",
    "            cols = [c for c in df.columns\n",
    "                   if any(kw in c.lower() for kw in keywords)\n",
    "                   and c not in config.ID_COLUMNS + [config.TARGET_COLUMN] + config.CATEGORICAL_FEATURES]\n",
    "\n",
    "            for col in cols:\n",
    "                if df[col].dtype in ['float64', 'float32', 'int64', 'int32', 'int16', 'int8']:\n",
    "                    Q1, Q3 = df[col].quantile([0.25, 0.75])\n",
    "                    IQR = Q3 - Q1\n",
    "                    self.outlier_bounds[col] = {\n",
    "                        'lower': Q1 - config.OUTLIER_IQR_MULTIPLIER * IQR,\n",
    "                        'upper': Q3 + config.OUTLIER_IQR_MULTIPLIER * IQR\n",
    "                    }\n",
    "\n",
    "            for col, bounds in self.outlier_bounds.items():\n",
    "                df[col] = df[col].clip(lower=bounds['lower'], upper=bounds['upper'])\n",
    "\n",
    "            print(f\"   Clipped: {len(self.outlier_bounds)} columns (IQR × {config.OUTLIER_IQR_MULTIPLIER})\")\n",
    "        else:\n",
    "            for col, bounds in self.outlier_bounds.items():\n",
    "                if col in df.columns:\n",
    "                    df[col] = df[col].clip(lower=bounds['lower'], upper=bounds['upper'])\n",
    "\n",
    "        return df\n",
    "\n",
    "    def _handle_missing(self, df, fit):\n",
    "        if fit:\n",
    "            print(\"\\n3. Handling missing values...\")\n",
    "            self.numeric_cols_for_imputation = [\n",
    "                c for c in df.select_dtypes(include=[np.number]).columns\n",
    "                if c not in config.ID_COLUMNS + [config.TARGET_COLUMN]\n",
    "            ]\n",
    "            self.categorical_cols_for_imputation = [\n",
    "                c for c in config.CATEGORICAL_FEATURES if c in df.columns\n",
    "            ]\n",
    "\n",
    "            self.numeric_imputer = SimpleImputer(strategy='median')\n",
    "            self.categorical_imputer = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "            if len(self.numeric_cols_for_imputation) > 0:\n",
    "                df[self.numeric_cols_for_imputation] = self.numeric_imputer.fit_transform(\n",
    "                    df[self.numeric_cols_for_imputation]\n",
    "                )\n",
    "\n",
    "            if len(self.categorical_cols_for_imputation) > 0:\n",
    "                df[self.categorical_cols_for_imputation] = self.categorical_imputer.fit_transform(\n",
    "                    df[self.categorical_cols_for_imputation]\n",
    "                )\n",
    "\n",
    "            print(f\"   Imputed: {len(self.numeric_cols_for_imputation)} numeric, \"\n",
    "                  f\"{len(self.categorical_cols_for_imputation)} categorical\")\n",
    "        else:\n",
    "            if len(self.numeric_cols_for_imputation) > 0:\n",
    "                present = [c for c in self.numeric_cols_for_imputation if c in df.columns]\n",
    "                if present:\n",
    "                    df[present] = self.numeric_imputer.transform(df[present])\n",
    "\n",
    "            if len(self.categorical_cols_for_imputation) > 0:\n",
    "                present = [c for c in self.categorical_cols_for_imputation if c in df.columns]\n",
    "                if present:\n",
    "                    df[present] = self.categorical_imputer.transform(df[present])\n",
    "\n",
    "        return df\n",
    "\n",
    "    def _remove_correlations(self, df, fit):\n",
    "        \"\"\"Удаление высоко коррелирующих признаков\"\"\"\n",
    "        if not config.REMOVE_HIGH_CORRELATIONS:\n",
    "            return df\n",
    "\n",
    "        if fit:\n",
    "            print(f\"\\n4. Removing high correlations (threshold={config.CORRELATION_THRESHOLD})...\")\n",
    "            numeric = [c for c in df.select_dtypes(include=[np.number]).columns\n",
    "                      if c not in config.ID_COLUMNS + [config.TARGET_COLUMN] + config.CATEGORICAL_FEATURES]\n",
    "\n",
    "            if len(numeric) > 1:\n",
    "                corr = df[numeric].corr().abs()\n",
    "                upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool))\n",
    "                self.features_to_drop_corr = [c for c in upper.columns\n",
    "                                             if any(upper[c] > config.CORRELATION_THRESHOLD)]\n",
    "\n",
    "                if self.features_to_drop_corr:\n",
    "                    df = df.drop(columns=self.features_to_drop_corr)\n",
    "                    print(f\"   Removed: {len(self.features_to_drop_corr)} features\")\n",
    "                else:\n",
    "                    print(f\"   ✓ No highly correlated features found\")\n",
    "\n",
    "        return df\n",
    "\n",
    "    def _align_columns(self, df, name):\n",
    "        \"\"\"Выравнивание колонок с train\"\"\"\n",
    "        preserve = [c for c in config.ID_COLUMNS if c in df.columns]\n",
    "        if config.TARGET_COLUMN in df.columns:\n",
    "            preserve.append(config.TARGET_COLUMN)\n",
    "\n",
    "        current = [c for c in df.columns if c not in preserve]\n",
    "        missing = [c for c in self.final_features if c not in current]\n",
    "        extra = [c for c in current if c not in self.final_features]\n",
    "\n",
    "        if missing:\n",
    "            for c in missing:\n",
    "                df[c] = 0\n",
    "\n",
    "        if extra:\n",
    "            df = df.drop(columns=extra)\n",
    "\n",
    "        order = preserve + self.final_features\n",
    "        df = df[[c for c in order if c in df.columns]]\n",
    "\n",
    "        return df\n",
    "\n",
    "print(\"✓ PreprocessingPipeline класс определен\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# ПРИМЕНЕНИЕ PREPROCESSING\n",
    "# ====================================================================================\n",
    "\n",
    "pipeline = PreprocessingPipeline(config)\n",
    "train_processed = pipeline.fit_transform(train_df)\n",
    "val_processed = pipeline.transform(val_df, 'validation')\n",
    "test_processed = pipeline.transform(test_df, 'test (OOT)')\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PREPROCESSING SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nШаги preprocessing:\")\n",
    "print(f\"  1. Константные колонки удалено: {len(pipeline.constant_cols)}\")\n",
    "print(f\"  2. Выбросы обработано (IQR clipping): {len(pipeline.outlier_bounds)} колонок\")\n",
    "print(f\"  3. Пропуски заполнено:\")\n",
    "print(f\"     - Числовых: {len(pipeline.numeric_cols_for_imputation)}\")\n",
    "print(f\"     - Категориальных: {len(pipeline.categorical_cols_for_imputation)}\")\n",
    "print(f\"  4. Коррелирующих признаков удалено: {len(pipeline.features_to_drop_corr)}\")\n",
    "print(f\"\\nИтоговое количество признаков: {len(pipeline.final_features)}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# РАЗДЕЛЕНИЕ ПО СЕГМЕНТАМ\n",
    "# ====================================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"РАЗДЕЛЕНИЕ ПО СЕГМЕНТАМ\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# SEGMENT 1: Small Business\n",
    "print(f\"\\n1. SEGMENT 1: {config.SEGMENT_1_NAME.upper()}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "seg1_train = train_processed[train_processed[config.SEGMENT_COLUMN].isin(config.SEGMENT_1_VALUES)].copy()\n",
    "seg1_val = val_processed[val_processed[config.SEGMENT_COLUMN].isin(config.SEGMENT_1_VALUES)].copy()\n",
    "seg1_test = test_processed[test_processed[config.SEGMENT_COLUMN].isin(config.SEGMENT_1_VALUES)].copy()\n",
    "\n",
    "print(f\"Исходные размеры:\")\n",
    "print(f\"  Train: {seg1_train.shape}\")\n",
    "print(f\"  Val: {seg1_val.shape}\")\n",
    "print(f\"  Test: {seg1_test.shape}\")\n",
    "\n",
    "# Удаляем segment_group для seg1 (там одно значение) + ID колонки + временные признаки\n",
    "print(f\"\\nУникальных значений segment_group: {seg1_train[config.SEGMENT_COLUMN].nunique()}\")\n",
    "print(f\"Значения: {seg1_train[config.SEGMENT_COLUMN].unique()}\")\n",
    "\n",
    "# Временные признаки вызывают высокий PSI (train vs test из разных периодов)\n",
    "temporal_features = ['obs_year', 'obs_month', 'obs_quarter']\n",
    "cols_to_drop_seg1 = [config.SEGMENT_COLUMN] + [c for c in config.ID_COLUMNS if c in seg1_train.columns] + temporal_features\n",
    "seg1_train = seg1_train.drop(columns=[c for c in cols_to_drop_seg1 if c in seg1_train.columns])\n",
    "seg1_val = seg1_val.drop(columns=[c for c in cols_to_drop_seg1 if c in seg1_val.columns])\n",
    "seg1_test = seg1_test.drop(columns=[c for c in cols_to_drop_seg1 if c in seg1_test.columns])\n",
    "\n",
    "print(f\"\\nИтоговые размеры seg1:\")\n",
    "print(f\"  Train: {seg1_train.shape} | Churn: {seg1_train[config.TARGET_COLUMN].mean()*100:.2f}%\")\n",
    "print(f\"  Val: {seg1_val.shape} | Churn: {seg1_val[config.TARGET_COLUMN].mean()*100:.2f}%\")\n",
    "print(f\"  Test: {seg1_test.shape} | Churn: {seg1_test[config.TARGET_COLUMN].mean()*100:.2f}%\")\n",
    "\n",
    "# SEGMENT 2: Middle + Large Business\n",
    "print(f\"\\n\\n2. SEGMENT 2: {config.SEGMENT_2_NAME.upper()}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "seg2_train = train_processed[train_processed[config.SEGMENT_COLUMN].isin(config.SEGMENT_2_VALUES)].copy()\n",
    "seg2_val = val_processed[val_processed[config.SEGMENT_COLUMN].isin(config.SEGMENT_2_VALUES)].copy()\n",
    "seg2_test = test_processed[test_processed[config.SEGMENT_COLUMN].isin(config.SEGMENT_2_VALUES)].copy()\n",
    "\n",
    "print(f\"Исходные размеры:\")\n",
    "print(f\"  Train: {seg2_train.shape}\")\n",
    "print(f\"  Val: {seg2_val.shape}\")\n",
    "print(f\"  Test: {seg2_test.shape}\")\n",
    "\n",
    "# Оставляем segment_group для seg2 (там два значения) + удаляем только ID + временные\n",
    "print(f\"\\nУникальных значений segment_group: {seg2_train[config.SEGMENT_COLUMN].nunique()}\")\n",
    "print(f\"Значения: {seg2_train[config.SEGMENT_COLUMN].unique()}\")\n",
    "\n",
    "cols_to_drop_seg2 = [c for c in config.ID_COLUMNS if c in seg2_train.columns] + temporal_features\n",
    "seg2_train = seg2_train.drop(columns=[c for c in cols_to_drop_seg2 if c in seg2_train.columns])\n",
    "seg2_val = seg2_val.drop(columns=[c for c in cols_to_drop_seg2 if c in seg2_val.columns])\n",
    "seg2_test = seg2_test.drop(columns=[c for c in cols_to_drop_seg2 if c in seg2_test.columns])\n",
    "\n",
    "# Преобразуем segment_group в числовой формат (Label Encoding)\n",
    "segment_mapping = {'MIDDLE_BUSINESS': 0, 'LARGE_BUSINESS': 1}\n",
    "seg2_train[config.SEGMENT_COLUMN] = seg2_train[config.SEGMENT_COLUMN].map(segment_mapping)\n",
    "seg2_val[config.SEGMENT_COLUMN] = seg2_val[config.SEGMENT_COLUMN].map(segment_mapping)\n",
    "seg2_test[config.SEGMENT_COLUMN] = seg2_test[config.SEGMENT_COLUMN].map(segment_mapping)\n",
    "print(f\"   MIDDLE_BUSINESS 0, LARGE_BUSINESS 1\")\n",
    "\n",
    "print(f\"\\nИтоговые размеры seg2:\")\n",
    "print(f\"  Train: {seg2_train.shape} | Churn: {seg2_train[config.TARGET_COLUMN].mean()*100:.2f}%\")\n",
    "print(f\"  Val: {seg2_val.shape} | Churn: {seg2_val[config.TARGET_COLUMN].mean()*100:.2f}%\")\n",
    "print(f\"  Test: {seg2_test.shape} | Churn: {seg2_test[config.TARGET_COLUMN].mean()*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# ФУНКЦИИ ДЛЯ CORRELATION ANALYSIS\n",
    "# ====================================================================================\n",
    "\n",
    "def calculate_pointbiserial_correlations(df, target_col, p_threshold=0.05):\n",
    "    # Получаем числовые колонки (кроме target)\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    numeric_cols = [c for c in numeric_cols if c != target_col]\n",
    "    \n",
    "    results = []\n",
    "    target_values = df[target_col].values\n",
    "    \n",
    "    print(f\"Анализируем {len(numeric_cols)} числовых признаков...\")\n",
    "    \n",
    "    for col in numeric_cols:\n",
    "        feature_values = df[col].values\n",
    "        \n",
    "        # Пропускаем константные колонки\n",
    "        if len(np.unique(feature_values)) == 1:\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            # Point-Biserial correlation\n",
    "            corr, pval = pointbiserialr(target_values, feature_values)\n",
    "            \n",
    "            results.append({\n",
    "                'feature': col,\n",
    "                'correlation': corr,\n",
    "                'abs_correlation': abs(corr),\n",
    "                'p_value': pval,\n",
    "                'significant': pval < p_threshold\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"  Ошибка для {col}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Создаем DataFrame\n",
    "    corr_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Сортируем по модулю корреляции\n",
    "    corr_df = corr_df.sort_values('abs_correlation', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    return corr_df\n",
    "\n",
    "\n",
    "def plot_top_correlations(corr_df, segment_name, top_n=30, save_path=None):\n",
    "    top_corr = corr_df.head(top_n).copy()\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, max(8, top_n * 0.3)))\n",
    "    \n",
    "    # Цвета: положительная - зеленый, отрицательная - красный\n",
    "    colors = ['green' if x >= 0 else 'red' for x in top_corr['correlation']]\n",
    "    \n",
    "    # Barplot\n",
    "    bars = ax.barh(range(len(top_corr)), top_corr['correlation'].values,\n",
    "                   color=colors, alpha=0.7, edgecolor='black')\n",
    "    \n",
    "    # Настройки\n",
    "    ax.set_yticks(range(len(top_corr)))\n",
    "    ax.set_yticklabels(top_corr['feature'].values, fontsize=9)\n",
    "    ax.set_xlabel('Point-Biserial Correlation', fontsize=12, fontweight='bold')\n",
    "    ax.set_title(f'Top-{top_n} Correlations with Target: {segment_name}',\n",
    "                fontsize=14, fontweight='bold', pad=20)\n",
    "    ax.axvline(x=0, color='black', linestyle='-', linewidth=0.8)\n",
    "    ax.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # Инвертируем ось Y чтобы самая высокая корреляция была сверху\n",
    "    ax.invert_yaxis()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "        print(f\"✓ Сохранено: {save_path}\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# CORRELATION ANALYSIS: SEGMENT 1\n",
    "# ====================================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"CORRELATION ANALYSIS: SEGMENT 1 - {config.SEGMENT_1_NAME.upper()}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Расчет корреляций на train\n",
    "print(f\"\\nРасчет Point-Biserial корреляций на train данных...\")\n",
    "start_time = time.time()\n",
    "\n",
    "corr_seg1 = calculate_pointbiserial_correlations(\n",
    "    seg1_train, \n",
    "    config.TARGET_COLUMN,\n",
    "    config.CORRELATION_P_VALUE_THRESHOLD\n",
    ")\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"\\n✓ Расчет завершен за {elapsed:.2f} сек\")\n",
    "\n",
    "# Статистика\n",
    "print(f\"\\nОБЩАЯ СТАТИСТИКА:\")\n",
    "print(f\"  Всего признаков: {len(corr_seg1)}\")\n",
    "print(f\"  Значимых (p<0.05): {corr_seg1['significant'].sum()}\")\n",
    "print(f\"  Средняя |корреляция|: {corr_seg1['abs_correlation'].mean():.4f}\")\n",
    "print(f\"  Максимальная |корреляция|: {corr_seg1['abs_correlation'].max():.4f}\")\n",
    "\n",
    "# Проверка на data leakage\n",
    "leakage_features = corr_seg1[corr_seg1['abs_correlation'] > config.DATA_LEAKAGE_THRESHOLD]\n",
    "if len(leakage_features) > 0:\n",
    "    print(f\"\\n  ВНИМАНИЕ: Обнаружены признаки с очень высокой корреляцией (>0.9):\")\n",
    "    print(leakage_features[['feature', 'correlation', 'p_value']].head(10))\n",
    "    print(f\"\\n Это может указывать на data leakage! Проверьте эти признаки.\")\n",
    "else:\n",
    "    print(f\"\\n Признаков с подозрением на data leakage не обнаружено\")\n",
    "\n",
    "# Топ-20 корреляций\n",
    "print(f\"\\nТОП-{config.TOP_N_CORRELATIONS} КОРРЕЛЯЦИЙ (по модулю):\")\n",
    "print(corr_seg1.head(config.TOP_N_CORRELATIONS)[['feature', 'correlation', 'p_value', 'significant']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# ВИЗУАЛИЗАЦИЯ: CORRELATION SEGMENT 1\n",
    "# ====================================================================================\n",
    "\n",
    "plot_top_correlations(\n",
    "    corr_seg1,\n",
    "    config.SEGMENT_1_NAME,\n",
    "    top_n=config.TOP_N_VISUALIZATION,\n",
    "    save_path=config.FIGURES_DIR / 'correlation_segment1.png'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# CORRELATION ANALYSIS: SEGMENT 2\n",
    "# ====================================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"CORRELATION ANALYSIS: SEGMENT 2 - {config.SEGMENT_2_NAME.upper()}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Расчет корреляций на train\n",
    "print(f\"\\nРасчет Point-Biserial корреляций на train данных...\")\n",
    "start_time = time.time()\n",
    "\n",
    "corr_seg2 = calculate_pointbiserial_correlations(\n",
    "    seg2_train, \n",
    "    config.TARGET_COLUMN,\n",
    "    config.CORRELATION_P_VALUE_THRESHOLD\n",
    ")\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"\\n✓ Расчет завершен за {elapsed:.2f} сек\")\n",
    "\n",
    "# Статистика\n",
    "print(f\"\\nОБЩАЯ СТАТИСТИКА:\")\n",
    "print(f\"  Всего признаков: {len(corr_seg2)}\")\n",
    "print(f\"  Значимых (p<0.05): {corr_seg2['significant'].sum()}\")\n",
    "print(f\"  Средняя |корреляция|: {corr_seg2['abs_correlation'].mean():.4f}\")\n",
    "print(f\"  Максимальная |корреляция|: {corr_seg2['abs_correlation'].max():.4f}\")\n",
    "\n",
    "# Проверка на data leakage\n",
    "leakage_features = corr_seg2[corr_seg2['abs_correlation'] > config.DATA_LEAKAGE_THRESHOLD]\n",
    "if len(leakage_features) > 0:\n",
    "    print(f\"\\n ВНИМАНИЕ: Обнаружены признаки с очень высокой корреляцией (>0.9):\")\n",
    "    print(leakage_features[['feature', 'correlation', 'p_value']].head(10))\n",
    "    print(f\"\\n Это может указывать на data leakage! Проверьте эти признаки.\")\n",
    "else:\n",
    "    print(f\"\\n✓ Признаков с подозрением на data leakage не обнаружено\")\n",
    "\n",
    "# Топ-20 корреляций\n",
    "print(f\"\\nТОП-{config.TOP_N_CORRELATIONS} КОРРЕЛЯЦИЙ (по модулю):\")\n",
    "print(corr_seg2.head(config.TOP_N_CORRELATIONS)[['feature', 'correlation', 'p_value', 'significant']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# СТАТИСТИКА ABT \n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"СТАТИСТИКА ИТОГОВОЙ ВИТРИНЫ ABT (Section 3.3)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "data = {\n",
    "    'Segment 1': {'train': seg1_train, 'val': seg1_val, 'test': seg1_test},\n",
    "    'Segment 2': {'train': seg2_train, 'val': seg2_val, 'test': seg2_test}\n",
    "}\n",
    "\n",
    "segments_info = {\n",
    "    'Segment 1': config.SEGMENT_1_NAME,\n",
    "    'Segment 2': config.SEGMENT_2_NAME\n",
    "}\n",
    "\n",
    "abt_statistics = {}\n",
    "\n",
    "for seg_id, seg_data in data.items():\n",
    "    print(f\"\\n{seg_id}: {segments_info[seg_id]}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    train_df = seg_data['train']\n",
    "    val_df = seg_data['val'] \n",
    "    test_df = seg_data['test']\n",
    "    \n",
    "    # Объединяем все splits\n",
    "    full_df = pd.concat([train_df, val_df, test_df], axis=0)\n",
    "    \n",
    "    # Разделяем на числовые и не числовые\n",
    "    numeric_cols = full_df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    non_numeric_cols = full_df.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "    \n",
    "    # Убираем target из предикторов\n",
    "    if config.TARGET_COLUMN in numeric_cols:\n",
    "        numeric_cols.remove(config.TARGET_COLUMN)\n",
    "    \n",
    "    stats = {\n",
    "        'Количество наблюдений': len(full_df),\n",
    "        'Количество событий (churn=1)': int(full_df[config.TARGET_COLUMN].sum()),\n",
    "        'Уровень целевой переменной (%)': f\"{full_df[config.TARGET_COLUMN].mean()*100:.2f}%\",\n",
    "        'Количество числовых предикторов': len(numeric_cols),\n",
    "        'Количество не числовых предикторов': len(non_numeric_cols),\n",
    "        'Всего признаков': len(numeric_cols) + len(non_numeric_cols),\n",
    "        'Train размер': len(train_df),\n",
    "        'Val размер': len(val_df),\n",
    "        'Test размер': len(test_df)\n",
    "    }\n",
    "    \n",
    "    abt_statistics[seg_id] = stats\n",
    "    \n",
    "    print(\"\\nСтатистика ABT:\")\n",
    "    print(\"-\" * 80)\n",
    "    for key, value in stats.items():\n",
    "        print(f\"  {key:45s}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PSI (POPULATION STABILITY INDEX)\n",
    "\n",
    "def calculate_psi(expected, actual, bins=10):\n",
    "    # Объединяем для определения границ бинов\n",
    "    combined = np.concatenate([expected, actual])\n",
    "    min_val = combined.min()\n",
    "    max_val = combined.max()\n",
    "    \n",
    "    # Создаем бины\n",
    "    breakpoints = np.linspace(min_val, max_val, bins + 1)\n",
    "    breakpoints[0] = -np.inf\n",
    "    breakpoints[-1] = np.inf\n",
    "    \n",
    "    # Распределения по бинам\n",
    "    expected_counts = np.histogram(expected, bins=breakpoints)[0]\n",
    "    actual_counts = np.histogram(actual, bins=breakpoints)[0]\n",
    "    \n",
    "    # Пропорции (с защитой от деления на 0)\n",
    "    expected_percents = expected_counts / len(expected)\n",
    "    actual_percents = actual_counts / len(actual)\n",
    "    \n",
    "    # Защита от нулевых значений (добавляем малое число)\n",
    "    expected_percents = np.where(expected_percents == 0, 0.0001, expected_percents)\n",
    "    actual_percents = np.where(actual_percents == 0, 0.0001, actual_percents)\n",
    "    \n",
    "    # PSI формула\n",
    "    psi_values = (actual_percents - expected_percents) * np.log(actual_percents / expected_percents)\n",
    "    psi = np.sum(psi_values)\n",
    "    \n",
    "    return psi\n",
    "\n",
    "def interpret_psi(psi_value):\n",
    "    \"\"\"Интерпретация значения PSI\"\"\"\n",
    "    if psi_value < 0.1:\n",
    "        return \"Отлично - модель стабильна\"\n",
    "    elif psi_value < 0.2:\n",
    "        return \"Приемлемо - небольшие изменения\"\n",
    "    else:\n",
    "        return \"Требует внимания - значительный drift\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PSI (POPULATION STABILITY INDEX) - TRAIN vs TEST\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "psi_results = {}\n",
    "\n",
    "# Используем словарь data (создан в ABT секции)\n",
    "for seg_id, seg_data in data.items():\n",
    "    print(f\"\\n{seg_id}: {segments_info[seg_id]}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Получаем данные\n",
    "    train_df = seg_data['train']\n",
    "    test_df = seg_data['test']\n",
    "    \n",
    "    X_train = train_df.drop(columns=[config.TARGET_COLUMN])\n",
    "    X_test = test_df.drop(columns=[config.TARGET_COLUMN])\n",
    "    \n",
    "    feature_psi = {}\n",
    "    \n",
    "    # Рассчитываем PSI для каждого признака\n",
    "    for col in X_train.columns:\n",
    "        try:\n",
    "            psi_val = calculate_psi(X_train[col].values, X_test[col].values)\n",
    "            feature_psi[col] = psi_val\n",
    "        except Exception as e:\n",
    "            feature_psi[col] = np.nan\n",
    "    \n",
    "    # Создаем DataFrame\n",
    "    psi_df = pd.DataFrame({\n",
    "        'Feature': list(feature_psi.keys()),\n",
    "        'PSI': list(feature_psi.values())\n",
    "    }).sort_values('PSI', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    psi_df['Interpretation'] = psi_df['PSI'].apply(interpret_psi)\n",
    "    \n",
    "    # Overall PSI (среднее по всем признакам)\n",
    "    overall_psi = psi_df['PSI'].mean()\n",
    "    \n",
    "    print(f\"\\nОбщий PSI (среднее по всем признакам): {overall_psi:.6f}\")\n",
    "    print(f\"Интерпретация: {interpret_psi(overall_psi)}\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\" * 80)\n",
    "    print(\"ТОП-15 признаков с наибольшим PSI:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(psi_df.head(15).to_string(index=False))\n",
    "    \n",
    "    # Статистика по категориям PSI\n",
    "    excellent = (psi_df['PSI'] < 0.1).sum()\n",
    "    acceptable = ((psi_df['PSI'] >= 0.1) & (psi_df['PSI'] < 0.2)).sum()\n",
    "    concerning = (psi_df['PSI'] >= 0.2).sum()\n",
    "    \n",
    "    print(\"\\n\" + \"-\" * 80)\n",
    "    print(\"Распределение признаков по PSI:\")\n",
    "    print(f\"Отличная стабильность (PSI < 0.1):     {excellent} признаков ({excellent/len(psi_df)*100:.1f}%)\")\n",
    "    print(f\"Приемлемая стабильность (0.1-0.2):    {acceptable} признаков ({acceptable/len(psi_df)*100:.1f}%)\")\n",
    "    print(f\"Требует внимания (PSI >= 0.2):         {concerning} признаков ({concerning/len(psi_df)*100:.1f}%)\")\n",
    "    \n",
    "    psi_results[seg_id] = {\n",
    "        'overall_psi': overall_psi,\n",
    "        'psi_df': psi_df,\n",
    "        'excellent': excellent,\n",
    "        'acceptable': acceptable,\n",
    "        'concerning': concerning\n",
    "    }\n",
    "    \n",
    "    # Сохраняем\n",
    "    seg_num = seg_id.split()[1]\n",
    "    psi_file = config.OUTPUT_DIR / f'psi_analysis_seg{seg_num}.csv'\n",
    "    psi_df.to_csv(psi_file, index=False)\n",
    "    print(f\"\\n✓ Сохранено: {psi_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# КОРРЕЛЯЦИОННЫЙ АНАЛИЗ И МУЛЬТИКОЛЛИНЕАРНОСТЬ\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"КОРРЕЛЯЦИОННЫЙ АНАЛИЗ И МУЛЬТИКОЛЛИНЕАРНОСТЬ\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "correlation_results = {}\n",
    "\n",
    "# Используем словарь data (создан в ABT секции)\n",
    "for seg_id, seg_data in data.items():\n",
    "    print(f\"\\n{seg_id}: {segments_info[seg_id]}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Получаем данные\n",
    "    train_df = seg_data['train']\n",
    "    X_train = train_df.drop(columns=[config.TARGET_COLUMN])\n",
    "    y_train = train_df[config.TARGET_COLUMN]\n",
    "    \n",
    "    # Корреляционная матрица\n",
    "    corr_matrix = X_train.corr()\n",
    "    \n",
    "    # 1. Корреляция с целевой переменной\n",
    "    print(\"\\n1. КОРРЕЛЯЦИЯ ПРИЗНАКОВ С ЦЕЛЕВОЙ ПЕРЕМЕННОЙ\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Добавляем target для расчета корреляции\n",
    "    temp_df = X_train.copy()\n",
    "    temp_df[config.TARGET_COLUMN] = y_train\n",
    "    \n",
    "    target_corr = temp_df.corr()[config.TARGET_COLUMN].drop(config.TARGET_COLUMN).abs().sort_values(ascending=False)\n",
    "    \n",
    "    print(\"\\nТОП-20 признаков с наибольшей корреляцией с target:\")\n",
    "    print(target_corr.head(20).to_string())\n",
    "    \n",
    "    # 2. Мультиколлинеарность (высокая корреляция между признаками)\n",
    "    print(\"\\n\\n2. МУЛЬТИКОЛЛИНЕАРНОСТЬ (высокая корреляция между признаками)\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Находим пары признаков с высокой корреляцией (> 0.8)\n",
    "    high_corr_threshold = 0.8\n",
    "    high_corr_pairs = []\n",
    "    \n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i+1, len(corr_matrix.columns)):\n",
    "            if abs(corr_matrix.iloc[i, j]) > high_corr_threshold:\n",
    "                high_corr_pairs.append({\n",
    "                    'Feature_1': corr_matrix.columns[i],\n",
    "                    'Feature_2': corr_matrix.columns[j],\n",
    "                    'Correlation': corr_matrix.iloc[i, j]\n",
    "                })\n",
    "    \n",
    "    if high_corr_pairs:\n",
    "        high_corr_df = pd.DataFrame(high_corr_pairs).sort_values('Correlation', ascending=False, key=abs)\n",
    "        print(f\"\\nНайдено {len(high_corr_df)} пар признаков с |correlation| > {high_corr_threshold}:\")\n",
    "        print(\"\\nТОП-20 пар с наибольшей корреляцией:\")\n",
    "        print(high_corr_df.head(20).to_string(index=False))\n",
    "    else:\n",
    "        print(f\"\\n Пар признаков с |correlation| > {high_corr_threshold} не найдено\")\n",
    "        print(\"   Мультиколлинеарность не является проблемой\")\n",
    "        high_corr_df = None\n",
    "    \n",
    "    # 3. Визуализация корреляционной матрицы (топ-30 признаков по корреляции с target)\n",
    "    print(\"\\n\\n3. ВИЗУАЛИЗАЦИЯ КОРРЕЛЯЦИОННОЙ МАТРИЦЫ\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Берем топ-30 признаков\n",
    "    top_features = target_corr.head(30).index.tolist()\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(14, 12))\n",
    "    \n",
    "    # Матрица корреляции для топ признаков\n",
    "    top_corr_matrix = X_train[top_features].corr()\n",
    "    \n",
    "    sns.heatmap(\n",
    "        top_corr_matrix,\n",
    "        cmap='coolwarm',\n",
    "        center=0,\n",
    "        annot=False,\n",
    "        fmt='.2f',\n",
    "        square=True,\n",
    "        linewidths=0.5,\n",
    "        cbar_kws={\"shrink\": 0.8},\n",
    "        ax=ax\n",
    "    )\n",
    "    \n",
    "    ax.set_title(f'Корреляционная матрица (ТОП-30 признаков) - {seg_id}', \n",
    "                 fontsize=14, fontweight='bold', pad=20)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    seg_num = seg_id.split()[1]\n",
    "    corr_plot_path = config.FIGURES_DIR / f'correlation_matrix_seg{seg_num}.png'\n",
    "    plt.savefig(corr_plot_path, dpi=100, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\n✓ Сохранено: {corr_plot_path}\")\n",
    "    \n",
    "    # 4. Сохранение результатов\n",
    "    correlation_results[seg_id] = {\n",
    "        'target_correlation': target_corr,\n",
    "        'high_corr_pairs': high_corr_df,\n",
    "        'corr_matrix': corr_matrix\n",
    "    }\n",
    "    \n",
    "    # Сохраняем CSV с корреляциями\n",
    "    target_corr_file = config.OUTPUT_DIR / f'target_correlation_seg{seg_num}.csv'\n",
    "    target_corr.to_csv(target_corr_file, header=['Correlation_with_Target'])\n",
    "    print(f\"✓ Сохранено: {target_corr_file}\")\n",
    "    \n",
    "    if high_corr_pairs:\n",
    "        high_corr_file = config.OUTPUT_DIR / f'high_correlation_pairs_seg{seg_num}.csv'\n",
    "        high_corr_df.to_csv(high_corr_file, index=False)\n",
    "        print(f\"✓ Сохранено: {high_corr_file}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✓ Корреляционный анализ завершен для всех сегментов\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# РАЗБИЕНИЕ ВЫБОРКИ - ТАБЛИЦА\n",
    "\n",
    "print(\"\\nМЕТОД РАЗБИЕНИЯ:\")\n",
    "print(\"-\" * 80)\n",
    "print(\"\\n✓ TEMPORAL SPLIT (по времени, не random!)\")\n",
    "print(\"  - Train: 70% первых наблюдений по времени\")\n",
    "print(\"  - Validation: 15% средних\")\n",
    "print(\"  - Test (OOT): 15% последних\")\n",
    "print(\"\\n✓ ОБОСНОВАНИЕ:\")\n",
    "print(\"  - Предотвращение data leakage\")\n",
    "print(\"  - Test = Out-of-Time validation (реальное будущее)\")\n",
    "print(\"  - Gap detection: удалены клиенты с пропусками в наблюдениях\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ТАБЛИЦА РАЗБИЕНИЯ ВЫБОРКИ\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "split_table_data = []\n",
    "\n",
    "for seg_id, seg_data in data.items():\n",
    "    for split_name in ['train', 'val', 'test']:\n",
    "        df = seg_data[split_name]\n",
    "        split_table_data.append({\n",
    "            'Сегмент': seg_id,\n",
    "            'Роль данных': split_name.upper(),\n",
    "            'Количество наблюдений': len(df),\n",
    "            'Количество событий (churn=1)': int(df[config.TARGET_COLUMN].sum()),\n",
    "            'Churn Rate (%)': f\"{df[config.TARGET_COLUMN].mean()*100:.2f}%\"\n",
    "        })\n",
    "\n",
    "split_table = pd.DataFrame(split_table_data)\n",
    "print(\"\\n\" + split_table.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ВСПОМОГАТЕЛЬНЫЕ ФУНКЦИИ ДЛЯ ОБУЧЕНИЯ\n",
    "\n",
    "def train_best_model(seg_id, algorithm, X_train, y_train, X_val, y_val, random_seed=42):\n",
    "    \"\"\"Обучить лучшую модель для сегмента\"\"\"\n",
    "    \n",
    "    if algorithm == 'XGBoost':\n",
    "        model = XGBClassifier(\n",
    "            n_estimators=300,\n",
    "            max_depth=6,\n",
    "            learning_rate=0.05,\n",
    "            objective='binary:logistic',\n",
    "            eval_metric='auc',\n",
    "            early_stopping_rounds=50,\n",
    "            random_state=random_seed,\n",
    "            n_jobs=-1,\n",
    "            verbosity=0\n",
    "        )\n",
    "        model.fit(\n",
    "            X_train, y_train,\n",
    "            eval_set=[(X_val, y_val)],\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "    elif algorithm == 'CatBoost':\n",
    "        model = CatBoostClassifier(\n",
    "            iterations=300,\n",
    "            depth=6,\n",
    "            learning_rate=0.05,\n",
    "            loss_function='Logloss',\n",
    "            eval_metric='AUC',\n",
    "            early_stopping_rounds=50,\n",
    "            use_best_model=True,\n",
    "            random_seed=random_seed,\n",
    "            task_type='CPU',\n",
    "            verbose=False,\n",
    "            allow_writing_files=False\n",
    "        )\n",
    "        train_pool = Pool(X_train, y_train)\n",
    "        val_pool = Pool(X_val, y_val)\n",
    "        model.fit(train_pool, eval_set=val_pool)\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown algorithm: {algorithm}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "def calculate_metrics(y_true, y_pred_proba, threshold=0.5):\n",
    "    \"\"\"Рассчитать все метрики\"\"\"\n",
    "    y_pred = (y_pred_proba >= threshold).astype(int)\n",
    "    \n",
    "    from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "    \n",
    "    metrics = {\n",
    "        'threshold': threshold,\n",
    "        'roc_auc': roc_auc_score(y_true, y_pred_proba),\n",
    "        'precision': precision_score(y_true, y_pred, zero_division=0),\n",
    "        'recall': recall_score(y_true, y_pred, zero_division=0),\n",
    "        'f1': f1_score(y_true, y_pred, zero_division=0)\n",
    "    }\n",
    "    \n",
    "    metrics['gini'] = 2 * metrics['roc_auc'] - 1\n",
    "    \n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    metrics['tn'], metrics['fp'] = cm[0, 0], cm[0, 1]\n",
    "    metrics['fn'], metrics['tp'] = cm[1, 0], cm[1, 1]\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ОБУЧЕНИЕ ЛУЧШИХ МОДЕЛЕЙ\n",
    "\n",
    "final_models = {}\n",
    "final_results = {}\n",
    "\n",
    "# Лучшие конфигурации (из экспериментов)\n",
    "best_configs = {\n",
    "    'Segment 1': {'algorithm': 'XGBoost', 'threshold': 0.12},\n",
    "    'Segment 2': {'algorithm': 'CatBoost', 'threshold': 0.10}\n",
    "}\n",
    "\n",
    "for seg_id, config_model in best_configs.items():\n",
    "    print(f\"\\n{seg_id}: {segments_info[seg_id]}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Подготовка данных\n",
    "    X_train = data[seg_id]['train'].drop(columns=[config.TARGET_COLUMN])\n",
    "    y_train = data[seg_id]['train'][config.TARGET_COLUMN]\n",
    "    \n",
    "    X_val = data[seg_id]['val'].drop(columns=[config.TARGET_COLUMN])\n",
    "    y_val = data[seg_id]['val'][config.TARGET_COLUMN]\n",
    "    \n",
    "    X_test = data[seg_id]['test'].drop(columns=[config.TARGET_COLUMN])\n",
    "    y_test = data[seg_id]['test'][config.TARGET_COLUMN]\n",
    "    \n",
    "    print(f\"  Algorithm: {config_model['algorithm']}\")\n",
    "    print(f\"  Train shape: {X_train.shape}\")\n",
    "    \n",
    "    # Обучение\n",
    "    print(f\"  Обучение...\")\n",
    "    import time\n",
    "    start = time.time()\n",
    "    \n",
    "    model = train_best_model(\n",
    "        seg_id,\n",
    "        config_model['algorithm'],\n",
    "        X_train, y_train,\n",
    "        X_val, y_val,\n",
    "        random_seed=config.RANDOM_SEED\n",
    "    )\n",
    "    \n",
    "    train_time = time.time() - start\n",
    "    print(f\"  ✓ Обучено за {train_time:.1f} сек\")\n",
    "    \n",
    "    # Предсказания\n",
    "    y_test_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Метрики\n",
    "    metrics = calculate_metrics(y_test, y_test_proba, config_model['threshold'])\n",
    "    \n",
    "    print(f\"\\n  МЕТРИКИ (TEST):\")\n",
    "    print(f\"    ROC-AUC:   {metrics['roc_auc']:.4f}\")\n",
    "    print(f\"    Gini:      {metrics['gini']:.4f}\")\n",
    "    print(f\"    F1:        {metrics['f1']:.4f}\")\n",
    "    print(f\"    Precision: {metrics['precision']:.4f}\")\n",
    "    print(f\"    Recall:    {metrics['recall']:.4f}\")\n",
    "    \n",
    "    # Сохранение\n",
    "    final_models[seg_id] = {\n",
    "        'model': model,\n",
    "        'X_train': X_train,\n",
    "        'X_test': X_test,\n",
    "        'y_test': y_test,\n",
    "        'y_test_proba': y_test_proba,\n",
    "        'algorithm': config_model['algorithm']\n",
    "    }\n",
    "    \n",
    "    final_results[seg_id] = {\n",
    "        'algorithm': config_model['algorithm'],\n",
    "        'train_time': train_time,\n",
    "        **metrics\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FEATURE IMPORTANCE\n",
    "\n",
    "for seg_id, model_data in final_models.items():\n",
    "    print(f\"\\n{seg_id}: {segments_info[seg_id]}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    model = model_data['model']\n",
    "    \n",
    "    # Get feature importance\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        importance = model.feature_importances_\n",
    "        feature_names = model_data['X_train'].columns\n",
    "        \n",
    "        # Create DataFrame\n",
    "        importance_df = pd.DataFrame({\n",
    "            'Feature': feature_names,\n",
    "            'Importance': importance\n",
    "        }).sort_values('Importance', ascending=False).reset_index(drop=True)\n",
    "        \n",
    "        print(f\"\\nТОП-20 признаков по важности:\")\n",
    "        print(\"-\" * 80)\n",
    "        print(importance_df.head(20).to_string(index=False))\n",
    "        \n",
    "        # Visualization\n",
    "        top20 = importance_df.head(20)\n",
    "        fig, ax = plt.subplots(figsize=(10, 8))\n",
    "        ax.barh(range(len(top20)), top20['Importance'], color='steelblue', alpha=0.7)\n",
    "        ax.set_yticks(range(len(top20)))\n",
    "        ax.set_yticklabels(top20['Feature'], fontsize=9)\n",
    "        ax.set_xlabel('Importance', fontsize=11, fontweight='bold')\n",
    "        ax.set_title(f'Feature Importance - {seg_id}', fontsize=13, fontweight='bold')\n",
    "        ax.invert_yaxis()\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        seg_num = seg_id.split()[1]\n",
    "        plt.savefig(config.FIGURES_DIR / f'feature_importance_seg{seg_num}.png', dpi=100)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# ROC CURVES - ФИНАЛЬНЫЕ МОДЕЛИ\n",
    "# ====================================================================================\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "colors = ['#2E86AB', '#A23B72']\n",
    "\n",
    "for idx, (seg_id, model_data) in enumerate(final_models.items()):\n",
    "    y_test = model_data['y_test']\n",
    "    y_pred_proba = model_data['y_test_proba']\n",
    "    \n",
    "    from sklearn.metrics import roc_curve\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "    roc_auc = final_results[seg_id]['roc_auc']\n",
    "    algorithm = model_data['algorithm']\n",
    "    \n",
    "    label = f\"{seg_id} | {algorithm} (AUC={roc_auc:.4f})\"\n",
    "    ax.plot(fpr, tpr, color=colors[idx], lw=2, label=label)\n",
    "\n",
    "ax.plot([0, 1], [0, 1], 'k--', lw=1, label='Random (AUC=0.5000)')\n",
    "ax.set_xlim([0.0, 1.0])\n",
    "ax.set_ylim([0.0, 1.05])\n",
    "ax.set_xlabel('False Positive Rate', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('True Positive Rate', fontsize=12, fontweight='bold')\n",
    "ax.set_title('ROC CURVES - ФИНАЛЬНЫЕ МОДЕЛИ', fontsize=14, fontweight='bold', pad=20)\n",
    "ax.legend(loc='lower right', fontsize=10)\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(config.FIGURES_DIR / 'final_roc_curves.png', dpi=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# СОХРАНЕНИЕ ФИНАЛЬНЫХ МОДЕЛЕЙ\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"СОХРАНЕНИЕ ФИНАЛЬНЫХ МОДЕЛЕЙ\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "models_dir = Path(\"models\")\n",
    "models_dir.mkdir(exist_ok=True)\n",
    "\n",
    "for seg_id, model_data in final_models.items():\n",
    "    seg_num = seg_id.split()[1]\n",
    "    algorithm = model_data['algorithm'].lower().replace(' ', '_')\n",
    "    \n",
    "    model_file = models_dir / f\"final_model_seg{seg_num}_{algorithm}.pkl\"\n",
    "    \n",
    "    with open(model_file, 'wb') as f:\n",
    "        pickle.dump(model_data['model'], f)\n",
    "    \n",
    "    file_size = model_file.stat().st_size / 1024\n",
    "    \n",
    "    print(f\"\\n✓ {model_file.name}\")\n",
    "    print(f\"  Сегмент: {seg_id}\")\n",
    "    print(f\"  Алгоритм: {model_data['algorithm']}\")\n",
    "    print(f\"  ROC-AUC: {final_results[seg_id]['roc_auc']:.4f}\")\n",
    "    print(f\"  Размер: {file_size:.1f} KB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenvc)",
   "language": "python",
   "name": "myenvc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
