{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4db8c9c-c723-4fa1-a6a0-d1d5216dfc48",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ðŸš€ ÐœÐÐ¡Ð¡ÐžÐ’ÐÐ¯ ÐšÐžÐÐ’Ð•Ð Ð¢ÐÐ¦Ð˜Ð¯ CSV â†’ PARQUET\n",
      "============================================================\n",
      "\n",
      "ÐÐ°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ¸:\n",
      "  Delimiter: '|'\n",
      "  Encoding: windows-1251\n",
      "  Categorical features: ['segment_group', 'obs_month', 'obs_quarter']\n",
      "  Compression: snappy\n",
      "\n",
      "\n",
      "############################################################\n",
      "# TRAINING DATASET\n",
      "############################################################\n",
      "\n",
      "============================================================\n",
      "ÐšÐžÐÐ’Ð•Ð Ð¢ÐÐ¦Ð˜Ð¯ CSV â†’ PARQUET Ð¡ ÐžÐŸÐ¢Ð˜ÐœÐ˜Ð—ÐÐ¦Ð˜Ð•Ð™\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Ð—ÐÐ“Ð Ð£Ð—ÐšÐ CSV: churn_train_ul.csv\n",
      "============================================================\n",
      "  Ð Ð°Ð·Ð¼ÐµÑ€ Ñ„Ð°Ð¹Ð»Ð°: 3100.61 MB\n",
      "\n",
      "  Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° CSV...\n",
      "  âœ“ CSV Ð·Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½ Ð·Ð° 104.26 ÑÐµÐº\n",
      "  Ð Ð°Ð·Ð¼ÐµÑ€ Ð´Ð°Ð½Ð½Ñ‹Ñ…: (3243871, 195)\n",
      "  ÐšÐ¾Ð»Ð¾Ð½ÐºÐ¸: 195\n",
      "\n",
      "  ÐžÐ¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ñ Ñ‚Ð¸Ð¿Ð¾Ð² Ð´Ð°Ð½Ð½Ñ‹Ñ…...\n",
      "    ÐŸÐ°Ð¼ÑÑ‚ÑŒ Ð´Ð¾ Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ð¸: 5194.33 MB\n",
      "    âœ“ segment_group: category (3 ÑƒÐ½Ð¸ÐºÐ°Ð»ÑŒÐ½Ñ‹Ñ…)\n",
      "    âœ“ obs_month: category (12 ÑƒÐ½Ð¸ÐºÐ°Ð»ÑŒÐ½Ñ‹Ñ…)\n",
      "    âœ“ obs_quarter: category (4 ÑƒÐ½Ð¸ÐºÐ°Ð»ÑŒÐ½Ñ‹Ñ…)\n",
      "    ÐŸÐ°Ð¼ÑÑ‚ÑŒ Ð¿Ð¾ÑÐ»Ðµ Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ð¸: 2051.06 MB\n",
      "    Ð­ÐºÐ¾Ð½Ð¾Ð¼Ð¸Ñ: 60.5%\n",
      "    ÐžÐ¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¾ ÐºÐ¾Ð»Ð¾Ð½Ð¾Ðº: 188\n",
      "\n",
      "============================================================\n",
      "Ð¡ÐžÐ¥Ð ÐÐÐ•ÐÐ˜Ð• Ð’ PARQUET\n",
      "============================================================\n",
      "  Ð¤Ð°Ð¹Ð»: churn_train_ul.parquet\n",
      "  Ð¡Ð¶Ð°Ñ‚Ð¸Ðµ: gzip\n",
      "  âœ“ Parquet ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½ Ð·Ð° 97.96 ÑÐµÐº\n",
      "  Ð Ð°Ð·Ð¼ÐµÑ€ Ñ„Ð°Ð¹Ð»Ð°: 747.39 MB\n",
      "\n",
      "============================================================\n",
      "Ð—ÐÐ“Ð Ð£Ð—ÐšÐ PARQUET: churn_train_ul.parquet\n",
      "============================================================\n",
      "  Ð Ð°Ð·Ð¼ÐµÑ€ Ñ„Ð°Ð¹Ð»Ð°: 747.39 MB\n",
      "\n",
      "  Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Parquet...\n",
      "  âœ“ Parquet Ð·Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½ Ð·Ð° 1.82 ÑÐµÐº\n",
      "  Ð Ð°Ð·Ð¼ÐµÑ€ Ð´Ð°Ð½Ð½Ñ‹Ñ…: (3243871, 195)\n",
      "  ÐŸÐ°Ð¼ÑÑ‚ÑŒ: 2094.37 MB\n",
      "\n",
      "============================================================\n",
      "ðŸ“Š Ð˜Ð¢ÐžÐ“ÐžÐ’ÐžÐ• Ð¡Ð ÐÐ’ÐÐ•ÐÐ˜Ð•\n",
      "============================================================\n",
      "\n",
      "ÐœÐµÑ‚Ñ€Ð¸ÐºÐ°                        CSV             Parquet         Ð£Ð»ÑƒÑ‡ÑˆÐµÐ½Ð¸Ðµ      \n",
      "---------------------------------------------------------------------------\n",
      "Ð Ð°Ð·Ð¼ÐµÑ€ Ñ„Ð°Ð¹Ð»Ð° (MB):             3100.61         747.39          4.1x Ð¼ÐµÐ½ÑŒÑˆÐµ\n",
      "Ð’Ñ€ÐµÐ¼Ñ Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ¸ (ÑÐµÐº):          104.26          1.82            57.4x Ð±Ñ‹ÑÑ‚Ñ€ÐµÐµ\n",
      "ÐŸÐ°Ð¼ÑÑ‚ÑŒ (MB):                   2051.06         2094.37         1.0x ÑÐºÐ¾Ð½Ð¾Ð¼Ð¸Ñ\n",
      "\n",
      "ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ñ†ÐµÐ»Ð¾ÑÑ‚Ð½Ð¾ÑÑ‚Ð¸:         \n",
      "  Ð Ð°Ð·Ð¼ÐµÑ€ ÑÐ¾Ð²Ð¿Ð°Ð´Ð°ÐµÑ‚: True\n",
      "  ÐšÐ¾Ð»Ð¾Ð½ÐºÐ¸ ÑÐ¾Ð²Ð¿Ð°Ð´Ð°ÑŽÑ‚: True\n",
      "\n",
      "ÐšÐ°Ñ‚ÐµÐ³Ð¾Ñ€Ð¸Ð°Ð»ÑŒÐ½Ñ‹Ðµ Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¸:      \n",
      "  segment_group: category (unique=3)\n",
      "  obs_month: int64 (unique=12)\n",
      "  obs_quarter: int64 (unique=4)\n",
      "\n",
      "Target Ñ€Ð°ÑÐ¿Ñ€ÐµÐ´ÐµÐ»ÐµÐ½Ð¸Ðµ:         \n",
      "  {0: 3195163, 1: 48708}\n",
      "\n",
      "\n",
      "############################################################\n",
      "# PRODUCTION DATASET\n",
      "############################################################\n",
      "\n",
      "============================================================\n",
      "ÐšÐžÐÐ’Ð•Ð Ð¢ÐÐ¦Ð˜Ð¯ CSV â†’ PARQUET Ð¡ ÐžÐŸÐ¢Ð˜ÐœÐ˜Ð—ÐÐ¦Ð˜Ð•Ð™\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Ð—ÐÐ“Ð Ð£Ð—ÐšÐ CSV: churn_prod_ul.csv\n",
      "============================================================\n",
      "  Ð Ð°Ð·Ð¼ÐµÑ€ Ñ„Ð°Ð¹Ð»Ð°: 195.22 MB\n",
      "\n",
      "  Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° CSV...\n",
      "  âœ“ CSV Ð·Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½ Ð·Ð° 6.45 ÑÐµÐº\n",
      "  Ð Ð°Ð·Ð¼ÐµÑ€ Ð´Ð°Ð½Ð½Ñ‹Ñ…: (206770, 194)\n",
      "  ÐšÐ¾Ð»Ð¾Ð½ÐºÐ¸: 194\n",
      "\n",
      "  ÐžÐ¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ñ Ñ‚Ð¸Ð¿Ð¾Ð² Ð´Ð°Ð½Ð½Ñ‹Ñ…...\n",
      "    ÐŸÐ°Ð¼ÑÑ‚ÑŒ Ð´Ð¾ Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ð¸: 329.52 MB\n",
      "    âœ“ segment_group: category (3 ÑƒÐ½Ð¸ÐºÐ°Ð»ÑŒÐ½Ñ‹Ñ…)\n",
      "    âœ“ obs_month: category (1 ÑƒÐ½Ð¸ÐºÐ°Ð»ÑŒÐ½Ñ‹Ñ…)\n",
      "    âœ“ obs_quarter: category (1 ÑƒÐ½Ð¸ÐºÐ°Ð»ÑŒÐ½Ñ‹Ñ…)\n",
      "    ÐŸÐ°Ð¼ÑÑ‚ÑŒ Ð¿Ð¾ÑÐ»Ðµ Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ð¸: 129.16 MB\n",
      "    Ð­ÐºÐ¾Ð½Ð¾Ð¼Ð¸Ñ: 60.8%\n",
      "    ÐžÐ¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¾ ÐºÐ¾Ð»Ð¾Ð½Ð¾Ðº: 188\n",
      "\n",
      "============================================================\n",
      "Ð¡ÐžÐ¥Ð ÐÐÐ•ÐÐ˜Ð• Ð’ PARQUET\n",
      "============================================================\n",
      "  Ð¤Ð°Ð¹Ð»: churn_prod_ul.parquet\n",
      "  Ð¡Ð¶Ð°Ñ‚Ð¸Ðµ: gzip\n",
      "  âœ“ Parquet ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½ Ð·Ð° 7.44 ÑÐµÐº\n",
      "  Ð Ð°Ð·Ð¼ÐµÑ€ Ñ„Ð°Ð¹Ð»Ð°: 54.24 MB\n",
      "\n",
      "============================================================\n",
      "Ð—ÐÐ“Ð Ð£Ð—ÐšÐ PARQUET: churn_prod_ul.parquet\n",
      "============================================================\n",
      "  Ð Ð°Ð·Ð¼ÐµÑ€ Ñ„Ð°Ð¹Ð»Ð°: 54.24 MB\n",
      "\n",
      "  Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Parquet...\n",
      "  âœ“ Parquet Ð·Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½ Ð·Ð° 0.18 ÑÐµÐº\n",
      "  Ð Ð°Ð·Ð¼ÐµÑ€ Ð´Ð°Ð½Ð½Ñ‹Ñ…: (206770, 194)\n",
      "  ÐŸÐ°Ð¼ÑÑ‚ÑŒ: 131.92 MB\n",
      "\n",
      "============================================================\n",
      "ðŸ“Š Ð˜Ð¢ÐžÐ“ÐžÐ’ÐžÐ• Ð¡Ð ÐÐ’ÐÐ•ÐÐ˜Ð•\n",
      "============================================================\n",
      "\n",
      "ÐœÐµÑ‚Ñ€Ð¸ÐºÐ°                        CSV             Parquet         Ð£Ð»ÑƒÑ‡ÑˆÐµÐ½Ð¸Ðµ      \n",
      "---------------------------------------------------------------------------\n",
      "Ð Ð°Ð·Ð¼ÐµÑ€ Ñ„Ð°Ð¹Ð»Ð° (MB):             195.22          54.24           3.6x Ð¼ÐµÐ½ÑŒÑˆÐµ\n",
      "Ð’Ñ€ÐµÐ¼Ñ Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ¸ (ÑÐµÐº):          6.45            0.18            36.4x Ð±Ñ‹ÑÑ‚Ñ€ÐµÐµ\n",
      "ÐŸÐ°Ð¼ÑÑ‚ÑŒ (MB):                   129.16          131.92          1.0x ÑÐºÐ¾Ð½Ð¾Ð¼Ð¸Ñ\n",
      "\n",
      "ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ñ†ÐµÐ»Ð¾ÑÑ‚Ð½Ð¾ÑÑ‚Ð¸:         \n",
      "  Ð Ð°Ð·Ð¼ÐµÑ€ ÑÐ¾Ð²Ð¿Ð°Ð´Ð°ÐµÑ‚: True\n",
      "  ÐšÐ¾Ð»Ð¾Ð½ÐºÐ¸ ÑÐ¾Ð²Ð¿Ð°Ð´Ð°ÑŽÑ‚: True\n",
      "\n",
      "ÐšÐ°Ñ‚ÐµÐ³Ð¾Ñ€Ð¸Ð°Ð»ÑŒÐ½Ñ‹Ðµ Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¸:      \n",
      "  segment_group: category (unique=3)\n",
      "  obs_month: int64 (unique=1)\n",
      "  obs_quarter: int64 (unique=1)\n",
      "\n",
      "\n",
      "============================================================\n",
      "âœ… ÐšÐžÐÐ’Ð•Ð Ð¢ÐÐ¦Ð˜Ð¯ Ð—ÐÐ’Ð•Ð Ð¨Ð•ÐÐ\n",
      "============================================================\n",
      "\n",
      "Dataset              Shape                Memory (MB)     Parquet Ð¿ÑƒÑ‚ÑŒ                  \n",
      "------------------------------------------------------------------------------------------\n",
      "Training             (3243871, 195)       2094.37         churn_train_ul.parquet        \n",
      "Production           (206770, 194)        131.92          churn_prod_ul.parquet         \n",
      "\n",
      "ðŸ’¡ Ð¢ÐµÐ¿ÐµÑ€ÑŒ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐ¹Ñ‚Ðµ pd.read_parquet() Ð´Ð»Ñ Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ¸ Ð´Ð°Ð½Ð½Ñ‹Ñ…!\n",
      "   ÐŸÑ€Ð¸Ð¼ÐµÑ€: df = pd.read_parquet('data/churn_train_ul.parquet')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "\n",
    "def optimize_dtypes(df, categorical_features, id_columns, target_column):\n",
    "    \"\"\"\n",
    "    ÐžÐ¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð¸Ñ€ÑƒÐµÑ‚ Ñ‚Ð¸Ð¿Ñ‹ Ð´Ð°Ð½Ð½Ñ‹Ñ… DataFrame\n",
    "    \"\"\"\n",
    "    print(\"\\n  ÐžÐ¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ñ Ñ‚Ð¸Ð¿Ð¾Ð² Ð´Ð°Ð½Ð½Ñ‹Ñ…...\")\n",
    "    memory_before = df.memory_usage(deep=True).sum() / (1024**2)\n",
    "    print(f\"    ÐŸÐ°Ð¼ÑÑ‚ÑŒ Ð´Ð¾ Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ð¸: {memory_before:.2f} MB\")\n",
    "\n",
    "    for col in categorical_features:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype('category')\n",
    "            print(f\"    âœ“ {col}: category ({df[col].nunique()} ÑƒÐ½Ð¸ÐºÐ°Ð»ÑŒÐ½Ñ‹Ñ…)\")\n",
    "\n",
    "    optimized_count = 0\n",
    "    for col in df.columns:\n",
    "        if col in id_columns + [target_column] + categorical_features:\n",
    "            continue\n",
    "\n",
    "        col_type = df[col].dtype\n",
    "\n",
    "        if col_type != 'object':\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                    optimized_count += 1\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                    optimized_count += 1\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                    optimized_count += 1\n",
    "\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                    optimized_count += 1\n",
    "\n",
    "    memory_after = df.memory_usage(deep=True).sum() / (1024**2)\n",
    "    savings = (1 - memory_after/memory_before) * 100\n",
    "\n",
    "    print(f\"    ÐŸÐ°Ð¼ÑÑ‚ÑŒ Ð¿Ð¾ÑÐ»Ðµ Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ð¸: {memory_after:.2f} MB\")\n",
    "    print(f\"    Ð­ÐºÐ¾Ð½Ð¾Ð¼Ð¸Ñ: {savings:.1f}%\")\n",
    "    print(f\"    ÐžÐ¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¾ ÐºÐ¾Ð»Ð¾Ð½Ð¾Ðº: {optimized_count}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_csv_with_original_settings(csv_path, delimiter='|', encoding='windows-1251'):\n",
    "    \"\"\"\n",
    "    Ð—Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÑ‚ CSV\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Ð—ÐÐ“Ð Ð£Ð—ÐšÐ CSV: {csv_path.name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    if not csv_path.exists():\n",
    "        raise FileNotFoundError(f\"Ð¤Ð°Ð¹Ð» Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½: {csv_path}\")\n",
    "\n",
    "    file_size = csv_path.stat().st_size / (1024**2)\n",
    "    print(f\"  Ð Ð°Ð·Ð¼ÐµÑ€ Ñ„Ð°Ð¹Ð»Ð°: {file_size:.2f} MB\")\n",
    "\n",
    "    print(f\"\\n  Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° CSV...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    df = pd.read_csv(\n",
    "        csv_path,\n",
    "        delimiter=delimiter,\n",
    "        encoding=encoding,\n",
    "        thousands=',',      # Ð’Ð°Ð¶Ð½Ð¾! Ñ‚Ð°Ðº ÐºÐ°Ðº Ñƒ Ð¼ÐµÐ½Ñ Ñ Ð±Ð¾Ð±Ñ€Ð° Ñ‚Ð°Ðº csv Ð²Ñ‹Ð³Ñ€ÑƒÐ·Ð¸Ð»ÑÑ, Ñ‡Ñ‚Ð¾ Ð½Ð°Ð´Ð¾ ÐµÑ‰Ðµ Ð¸ Ñ€Ð°Ð·Ð´ÐµÐ»ÐµÐ½Ð¸Ðµ Ñ‚Ñ‹ÑÑÑ‡Ð½Ñ‹Ñ… ÑƒÐºÐ°Ð·Ñ‹Ð²Ð°Ñ‚ÑŒ!\n",
    "        low_memory=False\n",
    "    )\n",
    "\n",
    "    load_time = time.time() - start_time\n",
    "\n",
    "    df.columns = df.columns.str.lower().str.strip()\n",
    "\n",
    "    print(f\"  âœ“ CSV Ð·Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½ Ð·Ð° {load_time:.2f} ÑÐµÐº\")\n",
    "    print(f\"  Ð Ð°Ð·Ð¼ÐµÑ€ Ð´Ð°Ð½Ð½Ñ‹Ñ…: {df.shape}\")\n",
    "    print(f\"  ÐšÐ¾Ð»Ð¾Ð½ÐºÐ¸: {len(df.columns)}\")\n",
    "\n",
    "    return df, load_time\n",
    "\n",
    "\n",
    "def save_to_parquet(df, parquet_path, compression='snappy'):\n",
    "    \"\"\"\n",
    "    Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÑ‚ DataFrame Ð² Parquet Ñ ÑÐ¶Ð°Ñ‚Ð¸ÐµÐ¼\n",
    "\n",
    "    ÐžÐ¿Ñ†Ð¸Ð¸ ÑÐ¶Ð°Ñ‚Ð¸Ñ:\n",
    "    - 'snappy': Ð±Ñ‹ÑÑ‚Ñ€Ð¾Ðµ ÑÐ¶Ð°Ñ‚Ð¸Ðµ (Ð¿Ð¾ ÑƒÐ¼Ð¾Ð»Ñ‡Ð°Ð½Ð¸ÑŽ)\n",
    "    - 'gzip': Ð»ÑƒÑ‡ÑˆÐµÐµ ÑÐ¶Ð°Ñ‚Ð¸Ðµ, Ð½Ð¾ Ð¼ÐµÐ´Ð»ÐµÐ½Ð½ÐµÐµ\n",
    "    - 'brotli': Ð¾Ñ‚Ð»Ð¸Ñ‡Ð½Ð¾Ðµ ÑÐ¶Ð°Ñ‚Ð¸Ðµ, Ð¼ÐµÐ´Ð»ÐµÐ½Ð½ÐµÐµ\n",
    "    - 'zstd': Ñ…Ð¾Ñ€Ð¾ÑˆÐ¸Ð¹ Ð±Ð°Ð»Ð°Ð½Ñ ÑÐºÐ¾Ñ€Ð¾ÑÑ‚Ð¸ Ð¸ ÑÐ¶Ð°Ñ‚Ð¸Ñ\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Ð¡ÐžÐ¥Ð ÐÐÐ•ÐÐ˜Ð• Ð’ PARQUET\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"  Ð¤Ð°Ð¹Ð»: {parquet_path.name}\")\n",
    "    print(f\"  Ð¡Ð¶Ð°Ñ‚Ð¸Ðµ: {compression}\")\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    df.to_parquet(\n",
    "        parquet_path,\n",
    "        engine='pyarrow',\n",
    "        compression=compression,\n",
    "        index=False\n",
    "    )\n",
    "\n",
    "    save_time = time.time() - start_time\n",
    "    file_size = parquet_path.stat().st_size / (1024**2)\n",
    "\n",
    "    print(f\"  âœ“ Parquet ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½ Ð·Ð° {save_time:.2f} ÑÐµÐº\")\n",
    "    print(f\"  Ð Ð°Ð·Ð¼ÐµÑ€ Ñ„Ð°Ð¹Ð»Ð°: {file_size:.2f} MB\")\n",
    "\n",
    "    return file_size, save_time\n",
    "\n",
    "\n",
    "def load_parquet(parquet_path):\n",
    "    \"\"\"\n",
    "    Ð—Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÑ‚ Parquet Ñ„Ð°Ð¹Ð»\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Ð—ÐÐ“Ð Ð£Ð—ÐšÐ PARQUET: {parquet_path.name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    if not parquet_path.exists():\n",
    "        raise FileNotFoundError(f\"Ð¤Ð°Ð¹Ð» Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½: {parquet_path}\")\n",
    "\n",
    "    file_size = parquet_path.stat().st_size / (1024**2)\n",
    "    print(f\"  Ð Ð°Ð·Ð¼ÐµÑ€ Ñ„Ð°Ð¹Ð»Ð°: {file_size:.2f} MB\")\n",
    "\n",
    "    print(f\"\\n  Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Parquet...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    df = pd.read_parquet(parquet_path)\n",
    "\n",
    "    load_time = time.time() - start_time\n",
    "\n",
    "    print(f\"  âœ“ Parquet Ð·Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½ Ð·Ð° {load_time:.2f} ÑÐµÐº\")\n",
    "    print(f\"  Ð Ð°Ð·Ð¼ÐµÑ€ Ð´Ð°Ð½Ð½Ñ‹Ñ…: {df.shape}\")\n",
    "    print(f\"  ÐŸÐ°Ð¼ÑÑ‚ÑŒ: {df.memory_usage(deep=True).sum() / (1024**2):.2f} MB\")\n",
    "\n",
    "    return df, load_time\n",
    "\n",
    "\n",
    "def convert_and_compare(csv_path, parquet_path,\n",
    "                       categorical_features, id_columns, target_column,\n",
    "                       delimiter='|', encoding='windows-1251',\n",
    "                       compression='snappy'):\n",
    "    \"\"\"\n",
    "    ÐŸÐ¾Ð»Ð½Ñ‹Ð¹ Ñ†Ð¸ÐºÐ»: CSV â†’ Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ñ â†’ Parquet â†’ Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð¾Ð±Ñ€Ð°Ñ‚Ð½Ð¾\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ÐšÐžÐÐ’Ð•Ð Ð¢ÐÐ¦Ð˜Ð¯ CSV â†’ PARQUET Ð¡ ÐžÐŸÐ¢Ð˜ÐœÐ˜Ð—ÐÐ¦Ð˜Ð•Ð™\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    df_csv, csv_load_time = load_csv_with_original_settings(\n",
    "        csv_path, delimiter, encoding\n",
    "    )\n",
    "    csv_size = csv_path.stat().st_size / (1024**2)\n",
    "\n",
    "    df_optimized = optimize_dtypes(\n",
    "        df_csv, categorical_features, id_columns, target_column\n",
    "    )\n",
    "\n",
    "    parquet_size, save_time = save_to_parquet(\n",
    "        df_optimized, parquet_path, compression\n",
    "    )\n",
    "\n",
    "    df_parquet, parquet_load_time = load_parquet(parquet_path)\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"ðŸ“Š Ð˜Ð¢ÐžÐ“ÐžÐ’ÐžÐ• Ð¡Ð ÐÐ’ÐÐ•ÐÐ˜Ð•\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    print(f\"\\n{'ÐœÐµÑ‚Ñ€Ð¸ÐºÐ°':<30} {'CSV':<15} {'Parquet':<15} {'Ð£Ð»ÑƒÑ‡ÑˆÐµÐ½Ð¸Ðµ':<15}\")\n",
    "    print(\"-\" * 75)\n",
    "\n",
    "    print(f\"{'Ð Ð°Ð·Ð¼ÐµÑ€ Ñ„Ð°Ð¹Ð»Ð° (MB):':<30} {csv_size:<15.2f} {parquet_size:<15.2f} {csv_size/parquet_size:.1f}x Ð¼ÐµÐ½ÑŒÑˆÐµ\")\n",
    "    print(f\"{'Ð’Ñ€ÐµÐ¼Ñ Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ¸ (ÑÐµÐº):':<30} {csv_load_time:<15.2f} {parquet_load_time:<15.2f} {csv_load_time/parquet_load_time:.1f}x Ð±Ñ‹ÑÑ‚Ñ€ÐµÐµ\")\n",
    "\n",
    "    csv_memory = df_csv.memory_usage(deep=True).sum() / (1024**2)\n",
    "    parquet_memory = df_parquet.memory_usage(deep=True).sum() / (1024**2)\n",
    "    print(f\"{'ÐŸÐ°Ð¼ÑÑ‚ÑŒ (MB):':<30} {csv_memory:<15.2f} {parquet_memory:<15.2f} {csv_memory/parquet_memory:.1f}x ÑÐºÐ¾Ð½Ð¾Ð¼Ð¸Ñ\")\n",
    "\n",
    "    print(f\"\\n{'ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ñ†ÐµÐ»Ð¾ÑÑ‚Ð½Ð¾ÑÑ‚Ð¸:':<30}\")\n",
    "    print(f\"  Ð Ð°Ð·Ð¼ÐµÑ€ ÑÐ¾Ð²Ð¿Ð°Ð´Ð°ÐµÑ‚: {df_csv.shape == df_parquet.shape}\")\n",
    "    print(f\"  ÐšÐ¾Ð»Ð¾Ð½ÐºÐ¸ ÑÐ¾Ð²Ð¿Ð°Ð´Ð°ÑŽÑ‚: {list(df_csv.columns) == list(df_parquet.columns)}\")\n",
    "\n",
    "    print(f\"\\n{'ÐšÐ°Ñ‚ÐµÐ³Ð¾Ñ€Ð¸Ð°Ð»ÑŒÐ½Ñ‹Ðµ Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¸:':<30}\")\n",
    "    for col in categorical_features:\n",
    "        if col in df_parquet.columns:\n",
    "            print(f\"  {col}: {df_parquet[col].dtype} (unique={df_parquet[col].nunique()})\")\n",
    "\n",
    "    if target_column in df_parquet.columns:\n",
    "        print(f\"\\n{'Target Ñ€Ð°ÑÐ¿Ñ€ÐµÐ´ÐµÐ»ÐµÐ½Ð¸Ðµ:':<30}\")\n",
    "        print(f\"  {df_parquet[target_column].value_counts().to_dict()}\")\n",
    "\n",
    "    return df_parquet\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    ÐžÑÐ½Ð¾Ð²Ð½Ð°Ñ Ñ„ÑƒÐ½ÐºÑ†Ð¸Ñ - ÐºÐ¾Ð½Ð²ÐµÑ€Ñ‚Ð°Ñ†Ð¸Ñ Ð²ÑÐµÑ… Ð´Ð°Ñ‚Ð°ÑÐµÑ‚Ð¾Ð²\n",
    "    \"\"\"\n",
    "    CATEGORICAL_FEATURES = ['segment_group', 'obs_month', 'obs_quarter']\n",
    "    ID_COLUMNS = ['cli_code', 'client_id', 'observation_point']\n",
    "    TARGET_COLUMN = 'target_churn_3m'\n",
    "    DELIMITER = '|'\n",
    "    ENCODING = 'windows-1251'\n",
    "\n",
    "    data_dir = Path(\"data\")\n",
    "\n",
    "    datasets = [\n",
    "        (data_dir / \"churn_train_ul.csv\", data_dir / \"churn_train_ul.parquet\", \"Training\"),\n",
    "        (data_dir / \"churn_prod_ul.csv\", data_dir / \"churn_prod_ul.parquet\", \"Production\"),\n",
    "    ]\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ðŸš€ ÐœÐÐ¡Ð¡ÐžÐ’ÐÐ¯ ÐšÐžÐÐ’Ð•Ð Ð¢ÐÐ¦Ð˜Ð¯ CSV â†’ PARQUET\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\nÐÐ°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ¸:\")\n",
    "    print(f\"  Delimiter: '{DELIMITER}'\")\n",
    "    print(f\"  Encoding: {ENCODING}\")\n",
    "    print(f\"  Categorical features: {CATEGORICAL_FEATURES}\")\n",
    "    print(f\"  Compression: snappy\")\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for csv_path, parquet_path, name in datasets:\n",
    "        if not csv_path.exists():\n",
    "            print(f\"\\nâš  ÐŸÑ€Ð¾Ð¿ÑƒÑ‰ÐµÐ½ {name}: Ñ„Ð°Ð¹Ð» {csv_path} Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n\\n{'#'*60}\")\n",
    "        print(f\"# {name.upper()} DATASET\")\n",
    "        print(f\"{'#'*60}\")\n",
    "\n",
    "        df = convert_and_compare(\n",
    "            csv_path=csv_path,\n",
    "            parquet_path=parquet_path,\n",
    "            categorical_features=CATEGORICAL_FEATURES,\n",
    "            id_columns=ID_COLUMNS,\n",
    "            target_column=TARGET_COLUMN,\n",
    "            delimiter=DELIMITER,\n",
    "            encoding=ENCODING,\n",
    "            compression='gzip'\n",
    "        )\n",
    "\n",
    "        results[name] = {\n",
    "            'csv': csv_path,\n",
    "            'parquet': parquet_path,\n",
    "            'shape': df.shape,\n",
    "            'memory': df.memory_usage(deep=True).sum() / (1024**2)\n",
    "        }\n",
    "\n",
    "    print(\"\\n\\n\" + \"=\"*60)\n",
    "    print(\"âœ… ÐšÐžÐÐ’Ð•Ð Ð¢ÐÐ¦Ð˜Ð¯ Ð—ÐÐ’Ð•Ð Ð¨Ð•ÐÐ\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    print(f\"\\n{'Dataset':<20} {'Shape':<20} {'Memory (MB)':<15} {'Parquet Ð¿ÑƒÑ‚ÑŒ':<30}\")\n",
    "    print(\"-\" * 90)\n",
    "\n",
    "    for name, info in results.items():\n",
    "        print(f\"{name:<20} {str(info['shape']):<20} {info['memory']:<15.2f} {info['parquet'].name:<30}\")\n",
    "\n",
    "    print(\"\\nðŸ’¡ Ð¢ÐµÐ¿ÐµÑ€ÑŒ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐ¹Ñ‚Ðµ pd.read_parquet() Ð´Ð»Ñ Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ¸ Ð´Ð°Ð½Ð½Ñ‹Ñ…!\")\n",
    "    print(\"   ÐŸÑ€Ð¸Ð¼ÐµÑ€: df = pd.read_parquet('data/churn_train_ul.parquet')\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e10934c-b3d0-4746-bc89-df013f82fde2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
