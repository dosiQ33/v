{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ИМПОРТ БИБЛИОТЕК И КОНФИГУРАЦИЯ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import time\n",
    "import gc\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from scipy.stats import pointbiserialr\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score, confusion_matrix, roc_curve\n",
    "\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.float_format', lambda x: '%.4f' % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    RANDOM_SEED = 42\n",
    "    \n",
    "    DATA_DIR = Path(\"data\")\n",
    "    OUTPUT_DIR = Path(\"output\")\n",
    "    FIGURES_DIR = Path(\"figures\")\n",
    "    MODELS_DIR = Path(\"models\")\n",
    "    \n",
    "    TRAIN_FILE = \"churn_train_ul.parquet\"\n",
    "    \n",
    "    ID_COLUMNS = ['cli_code', 'client_id', 'observation_point']\n",
    "    TARGET_COLUMN = 'target_churn_3m'\n",
    "    SEGMENT_COLUMN = 'segment_group'\n",
    "    DATE_COLUMN = 'observation_point'\n",
    "    CATEGORICAL_FEATURES = ['segment_group', 'obs_month', 'obs_quarter']\n",
    "    \n",
    "    SEGMENT_1_NAME = \"Small Business\"\n",
    "    SEGMENT_1_VALUES = ['SMALL_BUSINESS']\n",
    "    \n",
    "    SEGMENT_2_NAME = \"Middle + Large Business\"\n",
    "    SEGMENT_2_VALUES = ['MIDDLE_BUSINESS', 'LARGE_BUSINESS']\n",
    "    \n",
    "    TRAIN_SIZE = 0.70\n",
    "    VAL_SIZE = 0.15\n",
    "    TEST_SIZE = 0.15\n",
    "    \n",
    "    REMOVE_GAPS = True\n",
    "    HANDLE_OUTLIERS = True\n",
    "    REMOVE_HIGH_CORRELATIONS = True\n",
    "    CORRELATION_THRESHOLD = 0.85\n",
    "    OUTLIER_IQR_MULTIPLIER = 1.5\n",
    "    \n",
    "    CORRELATION_P_VALUE_THRESHOLD = 0.05\n",
    "    DATA_LEAKAGE_THRESHOLD = 0.9\n",
    "    TOP_N_CORRELATIONS = 20\n",
    "    TOP_N_VISUALIZATION = 30\n",
    "    \n",
    "    @classmethod\n",
    "    def create_directories(cls):\n",
    "        for dir_path in [cls.OUTPUT_DIR, cls.FIGURES_DIR, cls.MODELS_DIR]:\n",
    "            dir_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    @classmethod\n",
    "    def get_train_path(cls):\n",
    "        return cls.DATA_DIR / cls.TRAIN_FILE\n",
    "\n",
    "config = Config()\n",
    "config.create_directories()\n",
    "np.random.seed(config.RANDOM_SEED)\n",
    "\n",
    "print(\"\\nКонфигурация инициализирована\")\n",
    "print(f\"  Random seed: {config.RANDOM_SEED}\")\n",
    "print(f\"  Сегмент 1: {config.SEGMENT_1_NAME} {config.SEGMENT_1_VALUES}\")\n",
    "print(f\"  Сегмент 2: {config.SEGMENT_2_NAME} {config.SEGMENT_2_VALUES}\")\n",
    "print(f\"  Split: {config.TRAIN_SIZE}/{config.VAL_SIZE}/{config.TEST_SIZE}\")\n",
    "print(f\"\\nPREPROCESSING FLAGS:\")\n",
    "print(f\"  Gap detection: {config.REMOVE_GAPS}\")\n",
    "print(f\"  Outliers handling: {config.HANDLE_OUTLIERS} (IQR x {config.OUTLIER_IQR_MULTIPLIER})\")\n",
    "print(f\"  High correlations removal: {config.REMOVE_HIGH_CORRELATIONS} (threshold={config.CORRELATION_THRESHOLD})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ЗАГРУЗКА ДАННЫХ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = config.get_train_path()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ЗАГРУЗКА ДАННЫХ\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Файл: {train_path}\")\n",
    "\n",
    "if not train_path.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Файл не найден: {train_path}\\n\"\n",
    "        f\"Убедитесь, что файл {config.TRAIN_FILE} находится в папке data/\"\n",
    "    )\n",
    "\n",
    "file_size = train_path.stat().st_size / (1024**2)\n",
    "print(f\"Размер файла: {file_size:.2f} MB\")\n",
    "\n",
    "start = time.time()\n",
    "df_full = pd.read_parquet(train_path)\n",
    "load_time = time.time() - start\n",
    "\n",
    "memory = df_full.memory_usage(deep=True).sum() / (1024**2)\n",
    "\n",
    "print(f\"\\nЗагружено за {load_time:.2f} сек\")\n",
    "print(f\"  Размер: {df_full.shape}\")\n",
    "print(f\"  Память: {memory:.2f} MB\")\n",
    "print(f\"  Строк: {len(df_full):,}\")\n",
    "print(f\"  Колонок: {df_full.shape[1]}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXPLORATORY DATA ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"БАЗОВАЯ ИНФОРМАЦИЯ О ДАННЫХ\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n1. РАЗМЕР ДАННЫХ:\")\n",
    "print(f\"   Строк: {len(df_full):,}\")\n",
    "print(f\"   Колонок: {df_full.shape[1]}\")\n",
    "print(f\"   Память: {df_full.memory_usage(deep=True).sum() / (1024**2):.2f} MB\")\n",
    "\n",
    "print(f\"\\n2. ТИПЫ ДАННЫХ:\")\n",
    "dtype_counts = df_full.dtypes.value_counts()\n",
    "for dtype, count in dtype_counts.items():\n",
    "    print(f\"   {dtype}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"АНАЛИЗ ЦЕЛЕВОЙ ПЕРЕМЕННОЙ\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "churn_rate = df_full[config.TARGET_COLUMN].mean()\n",
    "n_churned = df_full[config.TARGET_COLUMN].sum()\n",
    "n_total = len(df_full)\n",
    "ratio = (1-churn_rate)/churn_rate\n",
    "\n",
    "print(f\"\\n1. ОБЩИЙ CHURN RATE:\")\n",
    "print(f\"   Churn rate: {churn_rate:.4f} ({churn_rate*100:.2f}%)\")\n",
    "print(f\"   Churned: {n_churned:,}\")\n",
    "print(f\"   Not churned: {n_total - n_churned:,}\")\n",
    "print(f\"   Class ratio: 1:{ratio:.1f}\")\n",
    "\n",
    "print(f\"\\n2. CHURN RATE ПО СЕГМЕНТАМ:\")\n",
    "for segment in df_full[config.SEGMENT_COLUMN].unique():\n",
    "    segment_df = df_full[df_full[config.SEGMENT_COLUMN] == segment]\n",
    "    seg_churn = segment_df[config.TARGET_COLUMN].mean()\n",
    "    seg_count = len(segment_df)\n",
    "    seg_pct = seg_count / len(df_full) * 100\n",
    "    print(f\"   {segment}:\")\n",
    "    print(f\"     Размер: {seg_count:,} ({seg_pct:.1f}%)\")\n",
    "    print(f\"     Churn rate: {seg_churn:.4f} ({seg_churn*100:.2f}%)\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"АНАЛИЗ ПРОПУЩЕННЫХ ЗНАЧЕНИЙ\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "missing = df_full.isnull().sum()\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing': missing[missing > 0],\n",
    "    'Percent': (missing[missing > 0] / len(df_full) * 100).round(2)\n",
    "}).sort_values('Missing', ascending=False)\n",
    "\n",
    "print(f\"\\nКолонок с пропусками: {len(missing_df)}\")\n",
    "\n",
    "if len(missing_df) > 0:\n",
    "    print(f\"\\nТоп-20 колонок с наибольшим количеством пропусков:\")\n",
    "    print(missing_df.head(20))\n",
    "else:\n",
    "    print(\"\\nПропущенных значений не обнаружено\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"АНАЛИЗ КОНСТАНТНЫХ КОЛОНОК\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "constant_cols = []\n",
    "for col in df_full.columns:\n",
    "    if df_full[col].nunique(dropna=False) == 1:\n",
    "        constant_cols.append(col)\n",
    "\n",
    "print(f\"\\nКонстантных колонок: {len(constant_cols)}\")\n",
    "\n",
    "if constant_cols:\n",
    "    print(f\"\\nСписок константных колонок:\")\n",
    "    for col in constant_cols:\n",
    "        print(f\"  - {col} (значение: {df_full[col].iloc[0]})\")\n",
    "else:\n",
    "    print(\"\\nКонстантных колонок не обнаружено\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"АНАЛИЗ ВРЕМЕННОГО РАСПРЕДЕЛЕНИЯ\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "df_full[config.DATE_COLUMN] = pd.to_datetime(df_full[config.DATE_COLUMN])\n",
    "\n",
    "print(f\"\\n1. ВРЕМЕННОЙ ПЕРИОД:\")\n",
    "print(f\"   Начало: {df_full[config.DATE_COLUMN].min().date()}\")\n",
    "print(f\"   Конец: {df_full[config.DATE_COLUMN].max().date()}\")\n",
    "print(f\"   Уникальных дат: {df_full[config.DATE_COLUMN].nunique()}\")\n",
    "\n",
    "print(f\"\\n2. КЛИЕНТЫ:\")\n",
    "print(f\"   Уникальных cli_code: {df_full['cli_code'].nunique():,}\")\n",
    "print(f\"   Уникальных client_id: {df_full['client_id'].nunique():,}\")\n",
    "\n",
    "print(f\"\\n3. РАСПРЕДЕЛЕНИЕ ЗАПИСЕЙ ПО ДАТАМ:\")\n",
    "date_dist = df_full.groupby(config.DATE_COLUMN).size()\n",
    "print(f\"   Среднее записей на дату: {date_dist.mean():.0f}\")\n",
    "print(f\"   Минимум: {date_dist.min():,}\")\n",
    "print(f\"   Максимум: {date_dist.max():,}\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "target_dist = df_full[config.TARGET_COLUMN].value_counts()\n",
    "axes[0].bar(['No Churn', 'Churn'], [target_dist[0], target_dist[1]],\n",
    "           color=['green', 'red'], alpha=0.7, edgecolor='black')\n",
    "axes[0].set_title('Общее распределение Target', fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylabel('Количество')\n",
    "axes[0].set_yscale('log')\n",
    "for i, v in enumerate([target_dist[0], target_dist[1]]):\n",
    "    axes[0].text(i, v, f'{v:,}\\n({v/len(df_full)*100:.2f}%)',\n",
    "                ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "segment_churn = df_full.groupby([config.SEGMENT_COLUMN, \n",
    "                                  config.TARGET_COLUMN]).size().unstack(fill_value=0)\n",
    "segment_churn.plot(kind='bar', stacked=True, ax=axes[1],\n",
    "                  color=['green', 'red'], alpha=0.7, edgecolor='black')\n",
    "axes[1].set_title('Распределение по сегментам', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Сегмент')\n",
    "axes[1].set_ylabel('Количество')\n",
    "axes[1].legend(['No Churn', 'Churn'], loc='upper right')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "churn_rates = df_full.groupby(config.SEGMENT_COLUMN)[config.TARGET_COLUMN].mean() * 100\n",
    "axes[2].bar(range(len(churn_rates)), churn_rates.values,\n",
    "           color='coral', alpha=0.7, edgecolor='black')\n",
    "axes[2].set_xticks(range(len(churn_rates)))\n",
    "axes[2].set_xticklabels(churn_rates.index, rotation=45, ha='right')\n",
    "axes[2].set_title('Churn Rate по сегментам', fontsize=14, fontweight='bold')\n",
    "axes[2].set_ylabel('Churn Rate (%)')\n",
    "for i, v in enumerate(churn_rates.values):\n",
    "    axes[2].text(i, v, f'{v:.2f}%', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(config.FIGURES_DIR / 'eda_target_distribution.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Сохранено: figures/eda_target_distribution.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ВРЕМЕННОЕ РАЗБИЕНИЕ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ВРЕМЕННОЕ РАЗБИЕНИЕ (TEMPORAL SPLIT)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "df_sorted = df_full.sort_values(config.DATE_COLUMN).reset_index(drop=True)\n",
    "unique_dates = sorted(df_sorted[config.DATE_COLUMN].unique())\n",
    "n_dates = len(unique_dates)\n",
    "\n",
    "print(f\"\\nУникальных дат: {n_dates}\")\n",
    "print(f\"Период: {unique_dates[0].date()} - {unique_dates[-1].date()}\")\n",
    "\n",
    "train_cutoff = int(n_dates * config.TRAIN_SIZE)\n",
    "val_cutoff = int(n_dates * (config.TRAIN_SIZE + config.VAL_SIZE))\n",
    "\n",
    "train_end = unique_dates[train_cutoff - 1]\n",
    "val_end = unique_dates[val_cutoff - 1]\n",
    "\n",
    "print(f\"\\nCutoff даты:\")\n",
    "print(f\"  Train: до {train_end.date()} ({train_cutoff} дат)\")\n",
    "print(f\"  Val: {unique_dates[train_cutoff].date()} - {val_end.date()} ({val_cutoff - train_cutoff} дат)\")\n",
    "print(f\"  Test (OOT): {unique_dates[val_cutoff].date()}+ ({n_dates - val_cutoff} дат)\")\n",
    "\n",
    "train_df = df_sorted[df_sorted[config.DATE_COLUMN] <= train_end].copy()\n",
    "val_df = df_sorted[(df_sorted[config.DATE_COLUMN] > train_end) & \n",
    "                   (df_sorted[config.DATE_COLUMN] <= val_end)].copy()\n",
    "test_df = df_sorted[df_sorted[config.DATE_COLUMN] > val_end].copy()\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"СТАТИСТИКА ПО SPLIT\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "for name, df in [('TRAIN', train_df), ('VALIDATION', val_df), ('TEST (OOT)', test_df)]:\n",
    "    churn_r = df[config.TARGET_COLUMN].mean()\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Записей: {len(df):,}\")\n",
    "    print(f\"  Клиентов (cli_code): {df['cli_code'].nunique():,}\")\n",
    "    print(f\"  Период: {df[config.DATE_COLUMN].min().date()} - {df[config.DATE_COLUMN].max().date()}\")\n",
    "    print(f\"  Churn rate: {churn_r:.4f} ({churn_r*100:.2f}%)\")\n",
    "    print(f\"  Процент от общего: {len(df)/len(df_full)*100:.2f}%\")\n",
    "\n",
    "assert train_df[config.DATE_COLUMN].max() < val_df[config.DATE_COLUMN].min(), \"Data leakage detected!\"\n",
    "assert val_df[config.DATE_COLUMN].max() < test_df[config.DATE_COLUMN].min(), \"Data leakage detected!\"\n",
    "print(f\"\\nTemporal ordering verified - NO DATA LEAKAGE\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.REMOVE_GAPS:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"GAP DETECTION - УДАЛЕНИЕ КЛИЕНТОВ С ПРОБЕЛАМИ\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    print(\"\\nАнализ пробелов в train данных...\")\n",
    "\n",
    "    unique_clients = train_df['cli_code'].unique()\n",
    "    chunk_size = 10000\n",
    "    clients_with_gaps_list = []\n",
    "\n",
    "    for i in range(0, len(unique_clients), chunk_size):\n",
    "        chunk_clients = unique_clients[i:i+chunk_size]\n",
    "        chunk = train_df[train_df['cli_code'].isin(chunk_clients)].copy()\n",
    "        chunk = chunk.sort_values(['cli_code', config.DATE_COLUMN])\n",
    "\n",
    "        chunk['month_num'] = chunk[config.DATE_COLUMN].dt.to_period('M').apply(lambda x: x.ordinal)\n",
    "        chunk['month_diff'] = chunk.groupby('cli_code')['month_num'].diff()\n",
    "\n",
    "        gaps = chunk.groupby('cli_code')['month_diff'].agg([\n",
    "            ('max_gap', 'max'),\n",
    "            ('total_gaps', lambda x: (x > 1).sum())\n",
    "        ]).reset_index()\n",
    "\n",
    "        chunk_gaps = gaps[gaps['max_gap'] > 1]\n",
    "        clients_with_gaps_list.append(chunk_gaps)\n",
    "\n",
    "        if (i // chunk_size + 1) % 10 == 0:\n",
    "            gc.collect()\n",
    "            print(f\"  Обработано {i+chunk_size:,}/{len(unique_clients):,} клиентов\")\n",
    "\n",
    "    clients_with_gaps = pd.concat(clients_with_gaps_list, ignore_index=True)\n",
    "\n",
    "    gap_pct = len(clients_with_gaps) / len(unique_clients) * 100\n",
    "    print(f\"\\nКлиентов с пробелами: {len(clients_with_gaps):,} ({gap_pct:.2f}%)\")\n",
    "\n",
    "    if len(clients_with_gaps) > 0:\n",
    "        bad_clients = set(clients_with_gaps['cli_code'])\n",
    "\n",
    "        train_before = len(train_df)\n",
    "        val_before = len(val_df)\n",
    "        test_before = len(test_df)\n",
    "        \n",
    "        train_df = train_df[~train_df['cli_code'].isin(bad_clients)].copy()\n",
    "        val_df = val_df[~val_df['cli_code'].isin(bad_clients)].copy()\n",
    "        test_df = test_df[~test_df['cli_code'].isin(bad_clients)].copy()\n",
    "\n",
    "        print(f\"\\nУдалено:\")\n",
    "        print(f\"  Train: {train_before:,} -> {len(train_df):,} (-{train_before - len(train_df):,})\")\n",
    "        print(f\"  Val: {val_before:,} -> {len(val_df):,} (-{val_before - len(val_df):,})\")\n",
    "        print(f\"  Test: {test_before:,} -> {len(test_df):,} (-{test_before - len(test_df):,})\")\n",
    "\n",
    "        del clients_with_gaps, bad_clients\n",
    "        gc.collect()\n",
    "    else:\n",
    "        print(\"\\nКлиентов с пробелами не обнаружено\")\n",
    "\n",
    "    print(\"=\"*80)\n",
    "else:\n",
    "    print(\"\\nGap detection отключен\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREPROCESSING PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreprocessingPipeline:\n",
    "\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.fitted_columns = None\n",
    "        self.final_features = None\n",
    "        self.constant_cols = []\n",
    "        self.outlier_bounds = {}\n",
    "        self.numeric_imputer = None\n",
    "        self.categorical_imputer = None\n",
    "        self.numeric_cols_for_imputation = []\n",
    "        self.categorical_cols_for_imputation = []\n",
    "        self.features_to_drop_corr = []\n",
    "\n",
    "    def fit_transform(self, train_df):\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"PREPROCESSING: FIT_TRANSFORM ON TRAIN\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "        df = train_df.copy()\n",
    "\n",
    "        self.fitted_columns = [c for c in df.columns\n",
    "                              if c not in config.ID_COLUMNS + [config.TARGET_COLUMN]]\n",
    "\n",
    "        df = self._remove_constants(df, fit=True)\n",
    "        df = self._handle_outliers(df, fit=True)\n",
    "        df = self._handle_missing(df, fit=True)\n",
    "        df = self._remove_correlations(df, fit=True)\n",
    "\n",
    "        self.final_features = [c for c in df.columns\n",
    "                              if c not in config.ID_COLUMNS + [config.TARGET_COLUMN]]\n",
    "\n",
    "        print(f\"\\nPreprocessing complete\")\n",
    "        print(f\"  Final features: {len(self.final_features)}\")\n",
    "\n",
    "        return df\n",
    "\n",
    "    def transform(self, df, dataset_name='test'):\n",
    "        print(f\"\\nPreprocessing: {dataset_name}\")\n",
    "\n",
    "        df = df.copy()\n",
    "\n",
    "        df = self._remove_constants(df, fit=False)\n",
    "        df = self._handle_outliers(df, fit=False)\n",
    "        df = self._handle_missing(df, fit=False)\n",
    "        df = self._remove_correlations(df, fit=False)\n",
    "        df = self._align_columns(df, dataset_name)\n",
    "\n",
    "        print(f\"  {dataset_name}: {df.shape}\")\n",
    "\n",
    "        return df\n",
    "\n",
    "    def _remove_constants(self, df, fit):\n",
    "        if fit:\n",
    "            print(\"\\n1. Removing constant columns...\")\n",
    "            for col in df.columns:\n",
    "                if col in config.ID_COLUMNS + [config.TARGET_COLUMN]:\n",
    "                    continue\n",
    "                if df[col].nunique(dropna=False) == 1:\n",
    "                    self.constant_cols.append(col)\n",
    "\n",
    "            if self.constant_cols:\n",
    "                df = df.drop(columns=self.constant_cols)\n",
    "                print(f\"   Removed: {len(self.constant_cols)}\")\n",
    "            else:\n",
    "                print(f\"   No constant columns found\")\n",
    "        return df\n",
    "\n",
    "    def _handle_outliers(self, df, fit):\n",
    "        if not config.HANDLE_OUTLIERS:\n",
    "            return df\n",
    "\n",
    "        if fit:\n",
    "            print(\"\\n2. Handling outliers (IQR clipping)...\")\n",
    "            keywords = ['profit', 'income', 'expense', 'margin', 'provision',\n",
    "                       'balance', 'assets', 'liabilities', 'revenue', 'cost']\n",
    "            \n",
    "            cols = [c for c in df.columns\n",
    "                   if any(kw in c.lower() for kw in keywords)\n",
    "                   and c not in config.ID_COLUMNS + [config.TARGET_COLUMN] + config.CATEGORICAL_FEATURES]\n",
    "\n",
    "            for col in cols:\n",
    "                if df[col].dtype in ['float64', 'float32', 'int64', 'int32', 'int16', 'int8']:\n",
    "                    Q1, Q3 = df[col].quantile([0.25, 0.75])\n",
    "                    IQR = Q3 - Q1\n",
    "                    self.outlier_bounds[col] = {\n",
    "                        'lower': Q1 - config.OUTLIER_IQR_MULTIPLIER * IQR,\n",
    "                        'upper': Q3 + config.OUTLIER_IQR_MULTIPLIER * IQR\n",
    "                    }\n",
    "\n",
    "            for col, bounds in self.outlier_bounds.items():\n",
    "                df[col] = df[col].clip(lower=bounds['lower'], upper=bounds['upper'])\n",
    "\n",
    "            print(f\"   Clipped: {len(self.outlier_bounds)} columns\")\n",
    "        else:\n",
    "            for col, bounds in self.outlier_bounds.items():\n",
    "                if col in df.columns:\n",
    "                    df[col] = df[col].clip(lower=bounds['lower'], upper=bounds['upper'])\n",
    "\n",
    "        return df\n",
    "\n",
    "    def _handle_missing(self, df, fit):\n",
    "        if fit:\n",
    "            print(\"\\n3. Handling missing values...\")\n",
    "            self.numeric_cols_for_imputation = [\n",
    "                c for c in df.select_dtypes(include=[np.number]).columns\n",
    "                if c not in config.ID_COLUMNS + [config.TARGET_COLUMN]\n",
    "            ]\n",
    "            self.categorical_cols_for_imputation = [\n",
    "                c for c in config.CATEGORICAL_FEATURES if c in df.columns\n",
    "            ]\n",
    "\n",
    "            self.numeric_imputer = SimpleImputer(strategy='median')\n",
    "            self.categorical_imputer = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "            if len(self.numeric_cols_for_imputation) > 0:\n",
    "                df[self.numeric_cols_for_imputation] = self.numeric_imputer.fit_transform(\n",
    "                    df[self.numeric_cols_for_imputation]\n",
    "                )\n",
    "\n",
    "            if len(self.categorical_cols_for_imputation) > 0:\n",
    "                df[self.categorical_cols_for_imputation] = self.categorical_imputer.fit_transform(\n",
    "                    df[self.categorical_cols_for_imputation]\n",
    "                )\n",
    "\n",
    "            print(f\"   Imputed: {len(self.numeric_cols_for_imputation)} numeric, \"\n",
    "                  f\"{len(self.categorical_cols_for_imputation)} categorical\")\n",
    "        else:\n",
    "            if len(self.numeric_cols_for_imputation) > 0:\n",
    "                present = [c for c in self.numeric_cols_for_imputation if c in df.columns]\n",
    "                if present:\n",
    "                    df[present] = self.numeric_imputer.transform(df[present])\n",
    "\n",
    "            if len(self.categorical_cols_for_imputation) > 0:\n",
    "                present = [c for c in self.categorical_cols_for_imputation if c in df.columns]\n",
    "                if present:\n",
    "                    df[present] = self.categorical_imputer.transform(df[present])\n",
    "\n",
    "        return df\n",
    "\n",
    "    def _remove_correlations(self, df, fit):\n",
    "        if not config.REMOVE_HIGH_CORRELATIONS:\n",
    "            return df\n",
    "\n",
    "        if fit:\n",
    "            print(f\"\\n4. Removing high correlations (threshold={config.CORRELATION_THRESHOLD})...\")\n",
    "            numeric = [c for c in df.select_dtypes(include=[np.number]).columns\n",
    "                      if c not in config.ID_COLUMNS + [config.TARGET_COLUMN] + config.CATEGORICAL_FEATURES]\n",
    "\n",
    "            if len(numeric) > 1:\n",
    "                corr = df[numeric].corr().abs()\n",
    "                upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool))\n",
    "                self.features_to_drop_corr = [c for c in upper.columns\n",
    "                                             if any(upper[c] > config.CORRELATION_THRESHOLD)]\n",
    "\n",
    "                if self.features_to_drop_corr:\n",
    "                    df = df.drop(columns=self.features_to_drop_corr)\n",
    "                    print(f\"   Removed: {len(self.features_to_drop_corr)} features\")\n",
    "                else:\n",
    "                    print(f\"   No highly correlated features found\")\n",
    "\n",
    "        return df\n",
    "\n",
    "    def _align_columns(self, df, name):\n",
    "        preserve = [c for c in config.ID_COLUMNS if c in df.columns]\n",
    "        if config.TARGET_COLUMN in df.columns:\n",
    "            preserve.append(config.TARGET_COLUMN)\n",
    "\n",
    "        current = [c for c in df.columns if c not in preserve]\n",
    "        missing = [c for c in self.final_features if c not in current]\n",
    "        extra = [c for c in current if c not in self.final_features]\n",
    "\n",
    "        if missing:\n",
    "            for c in missing:\n",
    "                df[c] = 0\n",
    "\n",
    "        if extra:\n",
    "            df = df.drop(columns=extra)\n",
    "\n",
    "        order = preserve + self.final_features\n",
    "        df = df[[c for c in order if c in df.columns]]\n",
    "\n",
    "        return df\n",
    "\n",
    "print(\"PreprocessingPipeline класс определен\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = PreprocessingPipeline(config)\n",
    "train_processed = pipeline.fit_transform(train_df)\n",
    "val_processed = pipeline.transform(val_df, 'validation')\n",
    "test_processed = pipeline.transform(test_df, 'test (OOT)')\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PREPROCESSING SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nШаги preprocessing:\")\n",
    "print(f\"  1. Константные колонки удалено: {len(pipeline.constant_cols)}\")\n",
    "print(f\"  2. Выбросы обработано (IQR clipping): {len(pipeline.outlier_bounds)} колонок\")\n",
    "print(f\"  3. Пропуски заполнено:\")\n",
    "print(f\"     - Числовых: {len(pipeline.numeric_cols_for_imputation)}\")\n",
    "print(f\"     - Категориальных: {len(pipeline.categorical_cols_for_imputation)}\")\n",
    "print(f\"  4. Коррелирующих признаков удалено: {len(pipeline.features_to_drop_corr)}\")\n",
    "print(f\"\\nИтоговое количество признаков: {len(pipeline.final_features)}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# РАЗДЕЛЕНИЕ ПО СЕГМЕНТАМ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"РАЗДЕЛЕНИЕ ПО СЕГМЕНТАМ\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n1. SEGMENT 1: {config.SEGMENT_1_NAME.upper()}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "seg1_train = train_processed[train_processed[config.SEGMENT_COLUMN].isin(config.SEGMENT_1_VALUES)].copy()\n",
    "seg1_val = val_processed[val_processed[config.SEGMENT_COLUMN].isin(config.SEGMENT_1_VALUES)].copy()\n",
    "seg1_test = test_processed[test_processed[config.SEGMENT_COLUMN].isin(config.SEGMENT_1_VALUES)].copy()\n",
    "\n",
    "print(f\"Исходные размеры:\")\n",
    "print(f\"  Train: {seg1_train.shape}\")\n",
    "print(f\"  Val: {seg1_val.shape}\")\n",
    "print(f\"  Test: {seg1_test.shape}\")\n",
    "\n",
    "print(f\"\\nУникальных значений segment_group: {seg1_train[config.SEGMENT_COLUMN].nunique()}\")\n",
    "print(f\"Значения: {seg1_train[config.SEGMENT_COLUMN].unique()}\")\n",
    "\n",
    "temporal_features = ['obs_year', 'obs_month', 'obs_quarter']\n",
    "cols_to_drop_seg1 = [config.SEGMENT_COLUMN] + [c for c in config.ID_COLUMNS if c in seg1_train.columns] + temporal_features\n",
    "seg1_train = seg1_train.drop(columns=[c for c in cols_to_drop_seg1 if c in seg1_train.columns])\n",
    "seg1_val = seg1_val.drop(columns=[c for c in cols_to_drop_seg1 if c in seg1_val.columns])\n",
    "seg1_test = seg1_test.drop(columns=[c for c in cols_to_drop_seg1 if c in seg1_test.columns])\n",
    "\n",
    "print(f\"\\nИтоговые размеры seg1:\")\n",
    "print(f\"  Train: {seg1_train.shape} | Churn: {seg1_train[config.TARGET_COLUMN].mean()*100:.2f}%\")\n",
    "print(f\"  Val: {seg1_val.shape} | Churn: {seg1_val[config.TARGET_COLUMN].mean()*100:.2f}%\")\n",
    "print(f\"  Test: {seg1_test.shape} | Churn: {seg1_test[config.TARGET_COLUMN].mean()*100:.2f}%\")\n",
    "\n",
    "print(f\"\\n\\n2. SEGMENT 2: {config.SEGMENT_2_NAME.upper()}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "seg2_train = train_processed[train_processed[config.SEGMENT_COLUMN].isin(config.SEGMENT_2_VALUES)].copy()\n",
    "seg2_val = val_processed[val_processed[config.SEGMENT_COLUMN].isin(config.SEGMENT_2_VALUES)].copy()\n",
    "seg2_test = test_processed[test_processed[config.SEGMENT_COLUMN].isin(config.SEGMENT_2_VALUES)].copy()\n",
    "\n",
    "print(f\"Исходные размеры:\")\n",
    "print(f\"  Train: {seg2_train.shape}\")\n",
    "print(f\"  Val: {seg2_val.shape}\")\n",
    "print(f\"  Test: {seg2_test.shape}\")\n",
    "\n",
    "print(f\"\\nУникальных значений segment_group: {seg2_train[config.SEGMENT_COLUMN].nunique()}\")\n",
    "print(f\"Значения: {seg2_train[config.SEGMENT_COLUMN].unique()}\")\n",
    "\n",
    "cols_to_drop_seg2 = [c for c in config.ID_COLUMNS if c in seg2_train.columns] + temporal_features\n",
    "seg2_train = seg2_train.drop(columns=[c for c in cols_to_drop_seg2 if c in seg2_train.columns])\n",
    "seg2_val = seg2_val.drop(columns=[c for c in cols_to_drop_seg2 if c in seg2_val.columns])\n",
    "seg2_test = seg2_test.drop(columns=[c for c in cols_to_drop_seg2 if c in seg2_test.columns])\n",
    "\n",
    "segment_mapping = {'MIDDLE_BUSINESS': 0, 'LARGE_BUSINESS': 1}\n",
    "seg2_train[config.SEGMENT_COLUMN] = seg2_train[config.SEGMENT_COLUMN].map(segment_mapping)\n",
    "seg2_val[config.SEGMENT_COLUMN] = seg2_val[config.SEGMENT_COLUMN].map(segment_mapping)\n",
    "seg2_test[config.SEGMENT_COLUMN] = seg2_test[config.SEGMENT_COLUMN].map(segment_mapping)\n",
    "print(f\"   MIDDLE_BUSINESS -> 0, LARGE_BUSINESS -> 1\")\n",
    "\n",
    "print(f\"\\nИтоговые размеры seg2:\")\n",
    "print(f\"  Train: {seg2_train.shape} | Churn: {seg2_train[config.TARGET_COLUMN].mean()*100:.2f}%\")\n",
    "print(f\"  Val: {seg2_val.shape} | Churn: {seg2_val[config.TARGET_COLUMN].mean()*100:.2f}%\")\n",
    "print(f\"  Test: {seg2_test.shape} | Churn: {seg2_test[config.TARGET_COLUMN].mean()*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CORRELATION ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_pointbiserial_correlations(df, target_col, p_threshold=0.05):\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    numeric_cols = [c for c in numeric_cols if c != target_col]\n",
    "    \n",
    "    results = []\n",
    "    target_values = df[target_col].values\n",
    "    \n",
    "    print(f\"Анализируем {len(numeric_cols)} числовых признаков...\")\n",
    "    \n",
    "    for col in numeric_cols:\n",
    "        feature_values = df[col].values\n",
    "        \n",
    "        if len(np.unique(feature_values)) == 1:\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            corr, pval = pointbiserialr(target_values, feature_values)\n",
    "            \n",
    "            results.append({\n",
    "                'feature': col,\n",
    "                'correlation': corr,\n",
    "                'abs_correlation': abs(corr),\n",
    "                'p_value': pval,\n",
    "                'significant': pval < p_threshold\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"  Ошибка для {col}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    corr_df = pd.DataFrame(results)\n",
    "    corr_df = corr_df.sort_values('abs_correlation', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    return corr_df\n",
    "\n",
    "\n",
    "def plot_top_correlations(corr_df, segment_name, top_n=30, save_path=None):\n",
    "    top_corr = corr_df.head(top_n).copy()\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, max(8, top_n * 0.3)))\n",
    "    \n",
    "    colors = ['green' if x >= 0 else 'red' for x in top_corr['correlation']]\n",
    "    \n",
    "    bars = ax.barh(range(len(top_corr)), top_corr['correlation'].values,\n",
    "                   color=colors, alpha=0.7, edgecolor='black')\n",
    "    \n",
    "    ax.set_yticks(range(len(top_corr)))\n",
    "    ax.set_yticklabels(top_corr['feature'].values, fontsize=9)\n",
    "    ax.set_xlabel('Point-Biserial Correlation', fontsize=12, fontweight='bold')\n",
    "    ax.set_title(f'Top-{top_n} Correlations with Target: {segment_name}',\n",
    "                fontsize=14, fontweight='bold', pad=20)\n",
    "    ax.axvline(x=0, color='black', linestyle='-', linewidth=0.8)\n",
    "    ax.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    ax.invert_yaxis()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "        print(f\"Сохранено: {save_path}\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"CORRELATION ANALYSIS: SEGMENT 1 - {config.SEGMENT_1_NAME.upper()}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nРасчет Point-Biserial корреляций на train данных...\")\n",
    "start_time = time.time()\n",
    "\n",
    "corr_seg1 = calculate_pointbiserial_correlations(\n",
    "    seg1_train, \n",
    "    config.TARGET_COLUMN,\n",
    "    config.CORRELATION_P_VALUE_THRESHOLD\n",
    ")\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"\\nРасчет завершен за {elapsed:.2f} сек\")\n",
    "\n",
    "print(f\"\\nОБЩАЯ СТАТИСТИКА:\")\n",
    "print(f\"  Всего признаков: {len(corr_seg1)}\")\n",
    "print(f\"  Значимых (p<0.05): {corr_seg1['significant'].sum()}\")\n",
    "print(f\"  Средняя |корреляция|: {corr_seg1['abs_correlation'].mean():.4f}\")\n",
    "print(f\"  Максимальная |корреляция|: {corr_seg1['abs_correlation'].max():.4f}\")\n",
    "\n",
    "leakage_features = corr_seg1[corr_seg1['abs_correlation'] > config.DATA_LEAKAGE_THRESHOLD]\n",
    "if len(leakage_features) > 0:\n",
    "    print(f\"\\n  ВНИМАНИЕ: Обнаружены признаки с очень высокой корреляцией (>0.9):\")\n",
    "    print(leakage_features[['feature', 'correlation', 'p_value']].head(10))\n",
    "    print(f\"\\n Это может указывать на data leakage\")\n",
    "else:\n",
    "    print(f\"\\n Признаков с подозрением на data leakage не обнаружено\")\n",
    "\n",
    "print(f\"\\nТОП-{config.TOP_N_CORRELATIONS} КОРРЕЛЯЦИЙ (по модулю):\")\n",
    "print(corr_seg1.head(config.TOP_N_CORRELATIONS)[['feature', 'correlation', 'p_value', 'significant']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_top_correlations(\n",
    "    corr_seg1,\n",
    "    config.SEGMENT_1_NAME,\n",
    "    top_n=config.TOP_N_VISUALIZATION,\n",
    "    save_path=config.FIGURES_DIR / 'correlation_segment1.png'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"CORRELATION ANALYSIS: SEGMENT 2 - {config.SEGMENT_2_NAME.upper()}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nРасчет Point-Biserial корреляций на train данных...\")\n",
    "start_time = time.time()\n",
    "\n",
    "corr_seg2 = calculate_pointbiserial_correlations(\n",
    "    seg2_train, \n",
    "    config.TARGET_COLUMN,\n",
    "    config.CORRELATION_P_VALUE_THRESHOLD\n",
    ")\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"\\nРасчет завершен за {elapsed:.2f} сек\")\n",
    "\n",
    "print(f\"\\nОБЩАЯ СТАТИСТИКА:\")\n",
    "print(f\"  Всего признаков: {len(corr_seg2)}\")\n",
    "print(f\"  Значимых (p<0.05): {corr_seg2['significant'].sum()}\")\n",
    "print(f\"  Средняя |корреляция|: {corr_seg2['abs_correlation'].mean():.4f}\")\n",
    "print(f\"  Максимальная |корреляция|: {corr_seg2['abs_correlation'].max():.4f}\")\n",
    "\n",
    "leakage_features = corr_seg2[corr_seg2['abs_correlation'] > config.DATA_LEAKAGE_THRESHOLD]\n",
    "if len(leakage_features) > 0:\n",
    "    print(f\"\\n ВНИМАНИЕ: Обнаружены признаки с очень высокой корреляцией (>0.9):\")\n",
    "    print(leakage_features[['feature', 'correlation', 'p_value']].head(10))\n",
    "    print(f\"\\n Это может указывать на data leakage\")\n",
    "else:\n",
    "    print(f\"\\n Признаков с подозрением на data leakage не обнаружено\")\n",
    "\n",
    "print(f\"\\nТОП-{config.TOP_N_CORRELATIONS} КОРРЕЛЯЦИЙ (по модулю):\")\n",
    "print(corr_seg2.head(config.TOP_N_CORRELATIONS)[['feature', 'correlation', 'p_value', 'significant']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_top_correlations(\n",
    "    corr_seg2,\n",
    "    config.SEGMENT_2_NAME,\n",
    "    top_n=config.TOP_N_VISUALIZATION,\n",
    "    save_path=config.FIGURES_DIR / 'correlation_segment2.png'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# СТАТИСТИКА ABT И PSI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"СТАТИСТИКА ИТОГОВОЙ ВИТРИНЫ ABT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "data = {\n",
    "    'Segment 1': {'train': seg1_train, 'val': seg1_val, 'test': seg1_test},\n",
    "    'Segment 2': {'train': seg2_train, 'val': seg2_val, 'test': seg2_test}\n",
    "}\n",
    "\n",
    "segments_info = {\n",
    "    'Segment 1': config.SEGMENT_1_NAME,\n",
    "    'Segment 2': config.SEGMENT_2_NAME\n",
    "}\n",
    "\n",
    "abt_statistics = {}\n",
    "\n",
    "for seg_id, seg_data in data.items():\n",
    "    print(f\"\\n{seg_id}: {segments_info[seg_id]}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    train_df = seg_data['train']\n",
    "    val_df = seg_data['val'] \n",
    "    test_df = seg_data['test']\n",
    "    \n",
    "    full_df = pd.concat([train_df, val_df, test_df], axis=0)\n",
    "    \n",
    "    numeric_cols = full_df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    non_numeric_cols = full_df.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "    \n",
    "    if config.TARGET_COLUMN in numeric_cols:\n",
    "        numeric_cols.remove(config.TARGET_COLUMN)\n",
    "    \n",
    "    stats = {\n",
    "        'Количество наблюдений': len(full_df),\n",
    "        'Количество событий (churn=1)': int(full_df[config.TARGET_COLUMN].sum()),\n",
    "        'Уровень целевой переменной (%)': f\"{full_df[config.TARGET_COLUMN].mean()*100:.2f}%\",\n",
    "        'Количество числовых предикторов': len(numeric_cols),\n",
    "        'Количество не числовых предикторов': len(non_numeric_cols),\n",
    "        'Всего признаков': len(numeric_cols) + len(non_numeric_cols),\n",
    "        'Train размер': len(train_df),\n",
    "        'Val размер': len(val_df),\n",
    "        'Test размер': len(test_df)\n",
    "    }\n",
    "    \n",
    "    abt_statistics[seg_id] = stats\n",
    "    \n",
    "    print(\"\\nСтатистика ABT:\")\n",
    "    print(\"-\" * 80)\n",
    "    for key, value in stats.items():\n",
    "        print(f\"  {key:45s}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_psi(expected, actual, bins=10):\n",
    "    combined = np.concatenate([expected, actual])\n",
    "    min_val = combined.min()\n",
    "    max_val = combined.max()\n",
    "    \n",
    "    breakpoints = np.linspace(min_val, max_val, bins + 1)\n",
    "    breakpoints[0] = -np.inf\n",
    "    breakpoints[-1] = np.inf\n",
    "    \n",
    "    expected_counts = np.histogram(expected, bins=breakpoints)[0]\n",
    "    actual_counts = np.histogram(actual, bins=breakpoints)[0]\n",
    "    \n",
    "    expected_percents = expected_counts / len(expected)\n",
    "    actual_percents = actual_counts / len(actual)\n",
    "    \n",
    "    expected_percents = np.where(expected_percents == 0, 0.0001, expected_percents)\n",
    "    actual_percents = np.where(actual_percents == 0, 0.0001, actual_percents)\n",
    "    \n",
    "    psi_values = (actual_percents - expected_percents) * np.log(actual_percents / expected_percents)\n",
    "    psi = np.sum(psi_values)\n",
    "    \n",
    "    return psi\n",
    "\n",
    "def interpret_psi(psi_value):\n",
    "    if psi_value < 0.1:\n",
    "        return \"Отлично - модель стабильна\"\n",
    "    elif psi_value < 0.2:\n",
    "        return \"Приемлемо - небольшие изменения\"\n",
    "    else:\n",
    "        return \"Требует внимания - значительный drift\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PSI (POPULATION STABILITY INDEX) - TRAIN vs TEST\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "psi_results = {}\n",
    "\n",
    "for seg_id, seg_data in data.items():\n",
    "    print(f\"\\n{seg_id}: {segments_info[seg_id]}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    train_df = seg_data['train']\n",
    "    test_df = seg_data['test']\n",
    "    \n",
    "    X_train = train_df.drop(columns=[config.TARGET_COLUMN])\n",
    "    X_test = test_df.drop(columns=[config.TARGET_COLUMN])\n",
    "    \n",
    "    feature_psi = {}\n",
    "    \n",
    "    for col in X_train.columns:\n",
    "        try:\n",
    "            psi_val = calculate_psi(X_train[col].values, X_test[col].values)\n",
    "            feature_psi[col] = psi_val\n",
    "        except Exception as e:\n",
    "            feature_psi[col] = np.nan\n",
    "    \n",
    "    psi_df = pd.DataFrame({\n",
    "        'Feature': list(feature_psi.keys()),\n",
    "        'PSI': list(feature_psi.values())\n",
    "    }).sort_values('PSI', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    psi_df['Interpretation'] = psi_df['PSI'].apply(interpret_psi)\n",
    "    \n",
    "    overall_psi = psi_df['PSI'].mean()\n",
    "    \n",
    "    print(f\"\\nОбщий PSI (среднее по всем признакам): {overall_psi:.6f}\")\n",
    "    print(f\"Интерпретация: {interpret_psi(overall_psi)}\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\" * 80)\n",
    "    print(\"ТОП-15 признаков с наибольшим PSI:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(psi_df.head(15).to_string(index=False))\n",
    "    \n",
    "    excellent = (psi_df['PSI'] < 0.1).sum()\n",
    "    acceptable = ((psi_df['PSI'] >= 0.1) & (psi_df['PSI'] < 0.2)).sum()\n",
    "    concerning = (psi_df['PSI'] >= 0.2).sum()\n",
    "    \n",
    "    print(\"\\n\" + \"-\" * 80)\n",
    "    print(\"Распределение признаков по PSI:\")\n",
    "    print(f\"Отличная стабильность (PSI < 0.1):     {excellent} признаков ({excellent/len(psi_df)*100:.1f}%)\")\n",
    "    print(f\"Приемлемая стабильность (0.1-0.2):    {acceptable} признаков ({acceptable/len(psi_df)*100:.1f}%)\")\n",
    "    print(f\"Требует внимания (PSI >= 0.2):         {concerning} признаков ({concerning/len(psi_df)*100:.1f}%)\")\n",
    "    \n",
    "    psi_results[seg_id] = {\n",
    "        'overall_psi': overall_psi,\n",
    "        'psi_df': psi_df,\n",
    "        'excellent': excellent,\n",
    "        'acceptable': acceptable,\n",
    "        'concerning': concerning\n",
    "    }\n",
    "    \n",
    "    seg_num = seg_id.split()[1]\n",
    "    psi_file = config.OUTPUT_DIR / f'psi_analysis_seg{seg_num}.csv'\n",
    "    psi_df.to_csv(psi_file, index=False)\n",
    "    print(f\"\\nСохранено: {psi_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nМЕТОД РАЗБИЕНИЯ:\")\n",
    "print(\"-\" * 80)\n",
    "print(\"\\nTEMPORAL SPLIT (по времени)\")\n",
    "print(\"  - Train: 70% первых наблюдений по времени\")\n",
    "print(\"  - Validation: 15% средних\")\n",
    "print(\"  - Test (OOT): 15% последних\")\n",
    "print(\"\\nОБОСНОВАНИЕ:\")\n",
    "print(\"  - Предотвращение data leakage\")\n",
    "print(\"  - Test = Out-of-Time validation (реальное будущее)\")\n",
    "print(\"  - Gap detection: удалены клиенты с пропусками в наблюдениях\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ТАБЛИЦА РАЗБИЕНИЯ ВЫБОРКИ\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "split_table_data = []\n",
    "\n",
    "for seg_id, seg_data in data.items():\n",
    "    for split_name in ['train', 'val', 'test']:\n",
    "        df = seg_data[split_name]\n",
    "        split_table_data.append({\n",
    "            'Сегмент': seg_id,\n",
    "            'Роль данных': split_name.upper(),\n",
    "            'Количество наблюдений': len(df),\n",
    "            'Количество событий (churn=1)': int(df[config.TARGET_COLUMN].sum()),\n",
    "            'Churn Rate (%)': f\"{df[config.TARGET_COLUMN].mean()*100:.2f}%\"\n",
    "        })\n",
    "\n",
    "split_table = pd.DataFrame(split_table_data)\n",
    "print(\"\\n\" + split_table.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ОБУЧЕНИЕ МОДЕЛЕЙ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(y_true, y_pred_proba, threshold=0.5):\n",
    "    y_pred = (y_pred_proba >= threshold).astype(int)\n",
    "    \n",
    "    metrics = {\n",
    "        'threshold': threshold,\n",
    "        'roc_auc': roc_auc_score(y_true, y_pred_proba),\n",
    "        'precision': precision_score(y_true, y_pred, zero_division=0),\n",
    "        'recall': recall_score(y_true, y_pred, zero_division=0),\n",
    "        'f1': f1_score(y_true, y_pred, zero_division=0)\n",
    "    }\n",
    "    \n",
    "    metrics['gini'] = 2 * metrics['roc_auc'] - 1\n",
    "    \n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    metrics['tn'], metrics['fp'] = cm[0, 0], cm[0, 1]\n",
    "    metrics['fn'], metrics['tp'] = cm[1, 0], cm[1, 1]\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "def train_catboost_classweight(X_train, y_train, X_val, y_val, random_seed=42):\n",
    "    pos_weight = (len(y_train) - y_train.sum()) / y_train.sum()\n",
    "    \n",
    "    model = CatBoostClassifier(\n",
    "        iterations=300,\n",
    "        depth=6,\n",
    "        learning_rate=0.05,\n",
    "        loss_function='Logloss',\n",
    "        eval_metric='AUC',\n",
    "        early_stopping_rounds=50,\n",
    "        use_best_model=True,\n",
    "        random_seed=random_seed,\n",
    "        task_type='CPU',\n",
    "        verbose=False,\n",
    "        allow_writing_files=False,\n",
    "        class_weights=[1, pos_weight]\n",
    "    )\n",
    "    \n",
    "    train_pool = Pool(X_train, y_train)\n",
    "    val_pool = Pool(X_val, y_val)\n",
    "    model.fit(train_pool, eval_set=val_pool)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def train_xgboost(X_train, y_train, X_val, y_val, random_seed=42):\n",
    "    model = XGBClassifier(\n",
    "        n_estimators=300,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.05,\n",
    "        objective='binary:logistic',\n",
    "        eval_metric='auc',\n",
    "        early_stopping_rounds=50,\n",
    "        random_state=random_seed,\n",
    "        n_jobs=-1,\n",
    "        verbosity=0\n",
    "    )\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        verbose=False\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_logistic(X_train, y_train, random_seed=42):\n",
    "    model = LogisticRegression(\n",
    "        max_iter=1000,\n",
    "        random_state=random_seed,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ОБУЧЕНИЕ МОДЕЛЕЙ С CLASSWEIGHT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "final_models = {}\n",
    "comparison_results = {}\n",
    "\n",
    "thresholds = {\n",
    "    'Segment 1': 0.12,\n",
    "    'Segment 2': 0.10\n",
    "}\n",
    "\n",
    "for seg_id, seg_data in data.items():\n",
    "    print(f\"\\n{seg_id}: {segments_info[seg_id]}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    X_train = seg_data['train'].drop(columns=[config.TARGET_COLUMN])\n",
    "    y_train = seg_data['train'][config.TARGET_COLUMN]\n",
    "    \n",
    "    X_val = seg_data['val'].drop(columns=[config.TARGET_COLUMN])\n",
    "    y_val = seg_data['val'][config.TARGET_COLUMN]\n",
    "    \n",
    "    X_test = seg_data['test'].drop(columns=[config.TARGET_COLUMN])\n",
    "    y_test = seg_data['test'][config.TARGET_COLUMN]\n",
    "    \n",
    "    print(f\"  Train shape: {X_train.shape}\")\n",
    "    print(f\"  Class ratio: 1:{(len(y_train) - y_train.sum()) / y_train.sum():.1f}\")\n",
    "    \n",
    "    seg_results = {}\n",
    "    \n",
    "    print(f\"\\n  Обучение CatBoost с class_weight...\")\n",
    "    start = time.time()\n",
    "    catboost_model = train_catboost_classweight(X_train, y_train, X_val, y_val, config.RANDOM_SEED)\n",
    "    catboost_time = time.time() - start\n",
    "    y_pred_catboost = catboost_model.predict_proba(X_test)[:, 1]\n",
    "    catboost_metrics = calculate_metrics(y_test, y_pred_catboost, thresholds[seg_id])\n",
    "    catboost_metrics['train_time'] = catboost_time\n",
    "    seg_results['CatBoost_classweight'] = catboost_metrics\n",
    "    print(f\"    Завершено за {catboost_time:.1f} сек | ROC-AUC: {catboost_metrics['roc_auc']:.4f}\")\n",
    "    \n",
    "    print(f\"  Обучение XGBoost...\")\n",
    "    start = time.time()\n",
    "    xgboost_model = train_xgboost(X_train, y_train, X_val, y_val, config.RANDOM_SEED)\n",
    "    xgboost_time = time.time() - start\n",
    "    y_pred_xgboost = xgboost_model.predict_proba(X_test)[:, 1]\n",
    "    xgboost_metrics = calculate_metrics(y_test, y_pred_xgboost, thresholds[seg_id])\n",
    "    xgboost_metrics['train_time'] = xgboost_time\n",
    "    seg_results['XGBoost'] = xgboost_metrics\n",
    "    print(f\"    Завершено за {xgboost_time:.1f} сек | ROC-AUC: {xgboost_metrics['roc_auc']:.4f}\")\n",
    "    \n",
    "    print(f\"  Обучение LogisticRegression...\")\n",
    "    start = time.time()\n",
    "    logistic_model = train_logistic(X_train, y_train, config.RANDOM_SEED)\n",
    "    logistic_time = time.time() - start\n",
    "    y_pred_logistic = logistic_model.predict_proba(X_test)[:, 1]\n",
    "    logistic_metrics = calculate_metrics(y_test, y_pred_logistic, thresholds[seg_id])\n",
    "    logistic_metrics['train_time'] = logistic_time\n",
    "    seg_results['LogisticRegression'] = logistic_metrics\n",
    "    print(f\"    Завершено за {logistic_time:.1f} сек | ROC-AUC: {logistic_metrics['roc_auc']:.4f}\")\n",
    "    \n",
    "    comparison_results[seg_id] = seg_results\n",
    "    \n",
    "    final_models[seg_id] = {\n",
    "        'model': catboost_model,\n",
    "        'X_train': X_train,\n",
    "        'X_test': X_test,\n",
    "        'y_test': y_test,\n",
    "        'y_test_proba': y_pred_catboost,\n",
    "        'algorithm': 'CatBoost_classweight',\n",
    "        'threshold': thresholds[seg_id]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"СРАВНЕНИЕ МОДЕЛЕЙ\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for seg_id, seg_results in comparison_results.items():\n",
    "    print(f\"\\n{seg_id}: {segments_info[seg_id]}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    comparison_df = pd.DataFrame([\n",
    "        {\n",
    "            'Model': model_name,\n",
    "            'ROC-AUC': metrics['roc_auc'],\n",
    "            'Gini': metrics['gini'],\n",
    "            'F1': metrics['f1'],\n",
    "            'Precision': metrics['precision'],\n",
    "            'Recall': metrics['recall'],\n",
    "            'Train Time (s)': metrics['train_time']\n",
    "        }\n",
    "        for model_name, metrics in seg_results.items()\n",
    "    ]).sort_values('ROC-AUC', ascending=False)\n",
    "    \n",
    "    print(\"\\n\" + comparison_df.to_string(index=False))\n",
    "    \n",
    "    seg_num = seg_id.split()[1]\n",
    "    comparison_file = config.OUTPUT_DIR / f'model_comparison_seg{seg_num}.csv'\n",
    "    comparison_df.to_csv(comparison_file, index=False)\n",
    "    print(f\"\\nСохранено: {comparison_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FEATURE IMPORTANCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seg_id, model_data in final_models.items():\n",
    "    print(f\"\\n{seg_id}: {segments_info[seg_id]}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    model = model_data['model']\n",
    "    \n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        importance = model.feature_importances_\n",
    "        feature_names = model_data['X_train'].columns\n",
    "        \n",
    "        importance_df = pd.DataFrame({\n",
    "            'Feature': feature_names,\n",
    "            'Importance': importance\n",
    "        }).sort_values('Importance', ascending=False).reset_index(drop=True)\n",
    "        \n",
    "        print(f\"\\nТОП-20 признаков по важности:\")\n",
    "        print(\"-\" * 80)\n",
    "        print(importance_df.head(20).to_string(index=False))\n",
    "        \n",
    "        top20 = importance_df.head(20)\n",
    "        fig, ax = plt.subplots(figsize=(10, 8))\n",
    "        ax.barh(range(len(top20)), top20['Importance'], color='steelblue', alpha=0.7)\n",
    "        ax.set_yticks(range(len(top20)))\n",
    "        ax.set_yticklabels(top20['Feature'], fontsize=9)\n",
    "        ax.set_xlabel('Importance', fontsize=11, fontweight='bold')\n",
    "        ax.set_title(f'Feature Importance - {seg_id}', fontsize=13, fontweight='bold')\n",
    "        ax.invert_yaxis()\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        seg_num = seg_id.split()[1]\n",
    "        plt.savefig(config.FIGURES_DIR / f'feature_importance_seg{seg_num}.png', dpi=100)\n",
    "        plt.show()\n",
    "        \n",
    "        importance_file = config.OUTPUT_DIR / f'feature_importance_seg{seg_num}.csv'\n",
    "        importance_df.to_csv(importance_file, index=False)\n",
    "        print(f\"\\nСохранено: {importance_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROC CURVES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "colors = ['#2E86AB', '#A23B72']\n",
    "\n",
    "for idx, (seg_id, model_data) in enumerate(final_models.items()):\n",
    "    y_test = model_data['y_test']\n",
    "    y_pred_proba = model_data['y_test_proba']\n",
    "    \n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    algorithm = model_data['algorithm']\n",
    "    \n",
    "    label = f\"{seg_id} | {algorithm} (AUC={roc_auc:.4f})\"\n",
    "    ax.plot(fpr, tpr, color=colors[idx], lw=2, label=label)\n",
    "\n",
    "ax.plot([0, 1], [0, 1], 'k--', lw=1, label='Random (AUC=0.5000)')\n",
    "ax.set_xlim([0.0, 1.0])\n",
    "ax.set_ylim([0.0, 1.05])\n",
    "ax.set_xlabel('False Positive Rate', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('True Positive Rate', fontsize=12, fontweight='bold')\n",
    "ax.set_title('ROC CURVES - ФИНАЛЬНЫЕ МОДЕЛИ', fontsize=14, fontweight='bold', pad=20)\n",
    "ax.legend(loc='lower right', fontsize=10)\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(config.FIGURES_DIR / 'final_roc_curves.png', dpi=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# СОХРАНЕНИЕ МОДЕЛЕЙ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"СОХРАНЕНИЕ ФИНАЛЬНЫХ МОДЕЛЕЙ\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for seg_id, model_data in final_models.items():\n",
    "    seg_num = seg_id.split()[1]\n",
    "    algorithm = model_data['algorithm'].lower().replace(' ', '_')\n",
    "    \n",
    "    model_file = config.MODELS_DIR / f\"final_model_seg{seg_num}_{algorithm}.pkl\"\n",
    "    \n",
    "    with open(model_file, 'wb') as f:\n",
    "        pickle.dump(model_data['model'], f)\n",
    "    \n",
    "    file_size = model_file.stat().st_size / 1024\n",
    "    \n",
    "    print(f\"\\n{model_file.name}\")\n",
    "    print(f\"  Сегмент: {seg_id}\")\n",
    "    print(f\"  Алгоритм: {model_data['algorithm']}\")\n",
    "    print(f\"  ROC-AUC: {roc_auc_score(model_data['y_test'], model_data['y_test_proba']):.4f}\")\n",
    "    print(f\"  Размер: {file_size:.1f} KB\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ВАЛИДАЦИЯ ЗАВЕРШЕНА\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenvc)",
   "language": "python",
   "name": "myenvc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
